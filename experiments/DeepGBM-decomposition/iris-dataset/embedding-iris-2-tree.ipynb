{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "41c45406",
    "execution_start": 1644068140043,
    "execution_millis": 707,
    "cell_id": "93b32f9d-5247-466d-8fa8-88535876ce1b",
    "deepnote_cell_type": "code"
   },
   "source": "import argparse, os, logging, random, time\nimport numpy as np\nimport math\nimport time\nimport scipy.sparse\nimport lightgbm as lgb\n# import data_helpers as dh\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "45134a31",
    "execution_start": 1644068140760,
    "execution_millis": 1683,
    "cell_id": "00001-e9095994-14e5-47c7-b828-86a241d003a4",
    "deepnote_cell_type": "code"
   },
   "source": "import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom sklearn.utils.extmath import softmax\n\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\nfrom torch.optim import Optimizer, AdamW\n\nimport gc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "58de1d54",
    "execution_start": 1644068142462,
    "execution_millis": 5,
    "deepnote_output_heights": [
     21
    ],
    "cell_id": "00002-d3096a14-35ca-4030-8424-0110a2925d0a",
    "deepnote_cell_type": "code"
   },
   "source": "torch.__version__",
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 3,
     "data": {
      "text/plain": "'1.10.0+cu102'"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "576d90f9",
    "execution_start": 1644068142471,
    "execution_millis": 5,
    "deepnote_output_heights": [
     21
    ],
    "cell_id": "00003-725226ee-011f-4ebd-910f-ebf9edeea7f9",
    "deepnote_cell_type": "code"
   },
   "source": "torchvision.__version__",
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 4,
     "data": {
      "text/plain": "'0.11.1+cu102'"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "280b5616",
    "execution_start": 1644068142480,
    "execution_millis": 3,
    "cell_id": "00004-9e880c51-03b3-4ca9-a8b1-248aae11ed5a",
    "deepnote_cell_type": "code"
   },
   "source": "import pdb\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif torch.cuda.is_available():\n    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n    type_prefix = torch.cuda\nelse:\n    type_prefix = torch",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "171b6a80",
    "execution_start": 1644068142529,
    "execution_millis": 1,
    "cell_id": "00005-67f97858-eccd-43e7-975a-3e77e38bc647",
    "deepnote_cell_type": "code"
   },
   "source": "from importlib import reload \n\nreload(logging)\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n# create file handler which logs even debug messages\nfh = logging.FileHandler('iris-2-emb.log')\nfh.setLevel(logging.INFO)\n# create console handler with a higher log level\nch = logging.StreamHandler()\nch.setLevel(logging.ERROR)\n# create formatter and add it to the handlers\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nch.setFormatter(formatter)\nfh.setFormatter(formatter)\n# add the handlers to logger\nlogger.addHandler(ch)\nlogger.addHandler(fh)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "47cd33c8",
    "execution_start": 1644068142531,
    "execution_millis": 123975,
    "cell_id": "00006-a255816b-2094-4faa-b39e-49260f0d1f6d",
    "deepnote_cell_type": "code"
   },
   "source": "def one_hot(y, numslot, mask=None):\n    y_tensor = y.type(type_prefix.LongTensor).reshape(-1, 1)\n    y_one_hot = torch.zeros(y_tensor.size()[0], numslot, device=device, dtype=torch.float32, requires_grad=False).scatter_(1, y_tensor, 1)\n    if mask is not None:\n        y_one_hot = y_one_hot * mask\n    y_one_hot = y_one_hot.reshape(y.shape[0], -1)\n    return y_one_hot",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ff6ce09a",
    "execution_start": 1644068142531,
    "execution_millis": 1,
    "cell_id": "00007-238a4d62-4a8f-40c2-9212-cb38789d3a55",
    "deepnote_cell_type": "code"
   },
   "source": "class BatchDense(nn.Module):\n    def __init__(self, batch, in_features, out_features, bias_init=None):\n        super(BatchDense, self).__init__()\n        self.batch = batch\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.Tensor(batch, in_features, out_features))\n        self.bias = Parameter(torch.Tensor(batch, 1, out_features))\n        self.reset_parameters(bias_init)\n    def reset_parameters(self, bias_init=None):\n        stdv = math.sqrt(6.0 /(self.in_features + self.out_features))\n        self.weight.data.uniform_(-stdv, stdv)\n        if bias_init is not None:\n            self.bias.data = torch.from_numpy(bias_init)\n        else:\n            self.bias.data.fill_(0)\n    def forward(self, x):\n        size = x.size()\n        # Todo: avoid the swap axis\n        x = x.view(x.size(0), self.batch, -1)\n        out = x.transpose(0, 1).contiguous()\n        out = torch.baddbmm(self.bias, out, self.weight)\n        out = out.transpose(0, 1).contiguous()\n        out = out.view(x.size(0), -1)\n        return out",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "18e69f81",
    "execution_start": 1644068142548,
    "execution_millis": 1,
    "cell_id": "00008-8264d20d-7269-4e43-8b10-a667a72ef525",
    "deepnote_cell_type": "code"
   },
   "source": "class EmbeddingModel(nn.Module):\n    def __init__(self, n_models, max_ntree_per_split, embsize, maxleaf, n_output, out_bias=None, task='regression'):\n        super(EmbeddingModel, self).__init__()\n        self.task = task\n        self.n_models = n_models\n        self.maxleaf = maxleaf\n        self.fcs = nn.ModuleList()\n        self.max_ntree_per_split = max_ntree_per_split\n\n        self.embed_w = Parameter(torch.Tensor(n_models, max_ntree_per_split*maxleaf, embsize))\n        # torch.nn.init.xavier_normal(self.embed_w)\n        stdv = math.sqrt(1.0 /(max_ntree_per_split))\n        self.embed_w.data.normal_(0,stdv) # .uniform_(-stdv, stdv)\n        \n        self.bout = BatchDense(n_models, embsize, 1, out_bias)\n        self.bn = nn.BatchNorm1d(embsize * n_models)\n        self.tanh = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n        # self.output_fc = Dense(n_models * embsize, n_output)\n        self.dropout = torch.nn.Dropout()\n        if task == 'regression':\n            self.criterion = nn.MSELoss()\n        else:\n            self.criterion = nn.BCELoss()\n\n    def batchmul(self, x, models, embed_w, length):\n        out = one_hot(x, length)\n        out = out.view(x.size(0), models, -1)\n        out = out.transpose(0, 1).contiguous()\n        out = torch.bmm(out, embed_w)\n        out = out.transpose(0, 1).contiguous()\n        out = out.view(x.size(0), -1)\n        return out\n        \n    def lastlayer(self, x):\n        out = self.batchmul(x, self.n_models, self.embed_w, self.maxleaf)\n        out = self.bn(out)\n        # out = self.tanh(out)\n        # out = out.view(x.size(0), self.n_models, -1)\n        return out\n    def forward(self, x):\n        out = self.lastlayer(x)\n        out = self.dropout(out)\n        out = out.view(x.size(0), self.n_models, -1)\n        out = self.bout(out)\n        # out = self.output_fc(out)\n        sum_out = torch.sum(out,-1,True)\n        if self.task != 'regression':\n            return self.sigmoid(sum_out), out\n        return sum_out, out\n    \n    def joint_loss(self, out, target, out_inner, target_inner, *args):\n        return nn.MSELoss()(out_inner, target_inner)\n\n    def true_loss(self, out, target):\n        return self.criterion(out, target)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f27db70a",
    "execution_start": 1644068142553,
    "execution_millis": 37,
    "cell_id": "00010-a8f32b5a-d3e8-4cc0-a48b-8dedac513d32",
    "deepnote_cell_type": "code"
   },
   "source": "def eval_metrics(task, true, pred):\n    if task == 'binary':\n        logloss = sklearn.metrics.log_loss(true.astype(np.float64), pred.astype(np.float64))\n        auc = sklearn.metrics.roc_auc_score(true, pred)\n        # error = 1-sklearn.metrics.accuracy_score(true,(pred+0.5).astype(np.int32))\n        return (logloss, auc)#, error)\n    else:\n        mseloss = sklearn.metrics.mean_squared_error(true, pred)\n        return mseloss\n\ndef EvalTestset(test_x, test_y, model, test_batch_size, test_x_opt=None):\n    test_len = test_x.shape[0]\n    test_num_batch = math.ceil(test_len / test_batch_size)\n    sum_loss = 0.0\n    y_preds = []\n    model.eval()\n    with torch.no_grad():\n        for jdx in range(test_num_batch):\n            tst_st = jdx * test_batch_size\n            tst_ed = min(test_len, tst_st + test_batch_size)\n            inputs = torch.from_numpy(test_x[tst_st:tst_ed].astype(np.float32)).to(device)\n            if test_x_opt is not None:\n                inputs_opt = torch.from_numpy(test_x_opt[tst_st:tst_ed].astype(np.float32)).to(device)\n                outputs = model(inputs, inputs_opt)\n            else:\n                outputs = model(inputs)\n            targets = torch.from_numpy(test_y[tst_st:tst_ed]).to(device)\n            if isinstance(outputs, tuple):\n                outputs = outputs[0]\n            y_preds.append(outputs)\n            loss_tst = model.true_loss(outputs, targets).item()\n            sum_loss += (tst_ed - tst_st) * loss_tst\n    return sum_loss / test_len, np.concatenate(y_preds, 0)\n\ndef TrainWithLog(loss_dr, loss_init, loss_de, log_freq, test_freq, task, test_batch_size,                \n                train_x, train_y, \n                 train_y_opt, test_x, test_y, model, opt,\n                 epoch, batch_size, n_output, key=\"\",\n                 train_x_opt=None, test_x_opt=None):\n    # trn_writer = tf.summary.FileWriter(summaryPath+plot_title+key+\"_output/train\")\n    # tst_writer = tf.summary.FileWriter(summaryPath+plot_title+key+\"_output/test\")\n    if isinstance(test_x, scipy.sparse.csr_matrix):\n        test_x = test_x.todense()\n    train_len = train_x.shape[0]\n    global_iter = 0\n    trn_batch_size = batch_size\n    train_num_batch = math.ceil(train_len / trn_batch_size)\n    total_iterations = epoch * train_num_batch\n    start_time = time.time()\n    total_time = 0.0\n    min_loss = float(\"Inf\")\n    # min_error = float(\"Inf\")\n    max_auc = 0.0\n    for epoch in range(epoch):\n        shuffled_indices = np.random.permutation(np.arange(train_x.shape[0]))\n        Loss_trn_epoch = 0.0\n        Loss_trn_log = 0.0\n        log_st = 0\n        for local_iter in range(train_num_batch):\n            trn_st = local_iter * trn_batch_size\n            trn_ed = min(train_len, trn_st + trn_batch_size)\n            batch_trn_x = train_x[shuffled_indices[trn_st:trn_ed]]\n            if isinstance(batch_trn_x, scipy.sparse.csr_matrix):\n                batch_trn_x = batch_trn_x.todense()\n            inputs = torch.from_numpy(batch_trn_x.astype(np.float32)).to(device)\n            targets = torch.from_numpy(train_y[shuffled_indices[trn_st:trn_ed],:]).to(device)\n            model.train()\n            if train_x_opt is not None:\n                inputs_opt = torch.from_numpy(train_x_opt[shuffled_indices[trn_st:trn_ed]].astype(np.float32)).to(device)\n                outputs = model(inputs, inputs_opt)\n            else:\n                outputs = model(inputs)\n            opt.zero_grad()\n            if isinstance(outputs, tuple) and train_y_opt is not None:\n                # targets_inner = torch.from_numpy(s_train_y_opt[trn_st:trn_ed,:]).to(device)\n                targets_inner = torch.from_numpy(train_y_opt[shuffled_indices[trn_st:trn_ed],:]).to(device)\n                loss_ratio = loss_init * max(0.3,loss_dr ** (epoch // loss_de))#max(0.5, args.loss_dr ** (epoch // args.loss_de))\n                if len(outputs) == 3:\n                    loss_val = model.joint_loss(outputs[0], targets, outputs[1], targets_inner, loss_ratio, outputs[2])\n                else:\n                    loss_val = model.joint_loss(outputs[0], targets, outputs[1], targets_inner, loss_ratio)\n                loss_val.backward()\n                loss_val = model.true_loss(outputs[0], targets)\n            elif isinstance(outputs, tuple):\n                loss_val = model.true_loss(outputs[0], targets)\n                loss_val.backward()\n            else:\n                loss_val = model.true_loss(outputs, targets)\n                loss_val.backward()\n            opt.step()\n            loss_val = loss_val.item()\n            global_iter += 1\n            Loss_trn_epoch += (trn_ed - trn_st) * loss_val\n            Loss_trn_log += (trn_ed - trn_st) * loss_val\n            if global_iter % log_freq == 0:\n                print(key+\"Epoch-{:0>3d} {:>5d} Batches, Step {:>6d}, Training Loss: {:>9.6f} (AllAvg {:>9.6f})\"\n                            .format(epoch, local_iter + 1, global_iter, Loss_trn_log/(trn_ed-log_st), Loss_trn_epoch/trn_ed))\n                \n                # trn_summ = tf.Summary()\n                # trn_summ.value.add(tag=args.data+ \"/Train/Loss\", simple_value = Loss_trn_log/(trn_ed-log_st))\n                # trn_writer.add_summary(trn_summ, global_iter)\n                log_st = trn_ed\n                Loss_trn_log = 0.0\n            if global_iter % test_freq == 0 or local_iter == train_num_batch - 1:\n                if model == 'deepgbm' or model == 'd1':\n                    try:\n                        print('Alpha: '+str(model.alpha))\n                        print('Beta: '+str(model.beta))\n                    except:\n                        pass\n                # tst_summ = tf.Summary()\n                torch.cuda.empty_cache()\n                test_loss, pred_y = EvalTestset(test_x, test_y, model, test_batch_size, test_x_opt)\n                current_used_time = time.time() - start_time\n                start_time = time.time()\n                total_time += current_used_time\n                remaining_time = (total_iterations - (global_iter) ) * (total_time / (global_iter))\n                if task == 'binary':\n                    metrics = eval_metrics(task, test_y, pred_y)\n                    _, test_auc = metrics\n                    # min_error = min(min_error, test_error)\n                    max_auc = max(max_auc, test_auc)\n                    # tst_summ.value.add(tag=args.data+\"/Test/Eval/Error\", simple_value = test_error)\n                    # tst_summ.value.add(tag=args.data+\"/Test/Eval/AUC\", simple_value = test_auc)\n                    # tst_summ.value.add(tag=args.data+\"/Test/Eval/Min_Error\", simple_value = min_error)\n                    # tst_summ.value.add(tag=args.data+\"/Test/Eval/Max_AUC\", simple_value = max_auc)\n                    print(key+\"Evaluate Result:\\nEpoch-{:0>3d} {:>5d} Batches, Step {:>6d}, Testing Loss: {:>9.6f}, Testing AUC: {:8.6f}, Used Time: {:>5.1f}m, Remaining Time: {:5.1f}m\"\n                            .format(epoch, local_iter + 1, global_iter, test_loss, test_auc, total_time/60.0, remaining_time/60.0))\n                else:\n                    print(key+\"Evaluate Result:\\nEpoch-{:0>3d} {:>5d} Batches, Step {:>6d}, Testing Loss: {:>9.6f}, Used Time: {:>5.1f}m, Remaining Time: {:5.1f}m\"\n                            .format(epoch, local_iter + 1, global_iter, test_loss, total_time/60.0, remaining_time/60.0))\n                min_loss = min(min_loss, test_loss)\n                # tst_summ.value.add(tag=args.data+\"/Test/Loss\", simple_value = test_loss)\n                # tst_summ.value.add(tag=args.data+\"/Test/Min_Loss\", simple_value = min_loss)\n                print(\"-------------------------------------------------------------------------------\")\n                # tst_writer.add_summary(tst_summ, global_iter)\n                # tst_writer.flush()\n        print(\"Best Metric: %s\"%(str(max_auc) if task=='binary' else str(min_loss)))\n        print(\"####################################################################################\")\n    print(\"Final Best Metric: %s\"%(str(max_auc) if task=='binary' else str(min_loss)))\n    return min_loss        \n\ndef GetEmbPred(model, fun, X, test_batch_size):\n    model.eval()\n    tst_len = X.shape[0]\n    test_num_batch = math.ceil(tst_len / test_batch_size)\n    y_preds = []\n    with torch.no_grad():\n        for jdx in range(test_num_batch):\n            tst_st = jdx * test_batch_size\n            tst_ed = min(tst_len, tst_st + test_batch_size)\n            inputs = torch.from_numpy(X[tst_st:tst_ed]).to(device)\n            t_preds = fun(inputs).data.cpu().numpy()\n            y_preds.append(t_preds)\n        y_preds = np.concatenate(y_preds, 0)\n    return y_preds\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "74b81f8d",
    "execution_start": 1644068745133,
    "execution_millis": 10,
    "cell_id": "00013-d65a9a7d-9ffb-46b5-a8ff-306d94220c42",
    "deepnote_cell_type": "code"
   },
   "source": "iris = datasets.load_iris()\nX = iris.data\ny = iris.target\ntrain_x, test_x, train_y, test_y = train_test_split(\n    X, y, test_size=0.33, random_state=42)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train_y = train_y.reshape(-1, 1)\ntest_y = test_y.reshape(-1, 1)",
   "metadata": {
    "cell_id": "9ab7b737-f747-4cbe-a830-12e89791f794",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "51e093d5",
    "execution_start": 1644069037259,
    "execution_millis": 0,
    "deepnote_output_heights": [
     21
    ],
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d2b7c5f4",
    "execution_start": 1644068142640,
    "execution_millis": 0,
    "cell_id": "00014-bd092f67-17cf-4260-a649-ebdc65386ff8",
    "deepnote_cell_type": "code"
   },
   "source": "import pickle\nwith open('n_models_iris_2.pickle', 'rb') as f:\n    # Pickle using the highest protocol available.\n    n_models = pickle.load(f)\n    \nwith open('max_ntree_per_split_iris_2.pickle', 'rb') as f:\n    # Pickle using the highest protocol available.\n    max_ntree_per_split = pickle.load(f)\n    \nwith open('group_average_iris_2.pickle', 'rb') as f:\n    # Pickle using the highest protocol available.\n    group_average = pickle.load(f)\n\nwith open('leaf_preds_iris_2.pickle', 'rb') as f:\n    # Pickle using the highest protocol available.\n    leaf_preds = pickle.load(f)\n    \nwith open('test_leaf_preds_iris_2.pickle', 'rb') as f:\n    # Pickle using the highest protocol available.\n    test_leaf_preds = pickle.load(f)\n    \nwith open('tree_outputs_iris_2.pickle', 'rb') as f:\n    # Pickle using the highest protocol available.\n    tree_outputs = pickle.load(f) \n    ",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1d8d693c",
    "execution_start": 1644069053898,
    "execution_millis": 299,
    "deepnote_output_heights": [
     345
    ],
    "cell_id": "00015-34aa4469-fea6-4b23-93b9-7c9bfab3314d",
    "deepnote_cell_type": "code"
   },
   "source": "embsize = 3\nmaxleaf = 4\ntask = \"regression\"\nl2_reg = 1e-6\nemb_lr = 1e-3\nemb_epoch = 3\nbatch_size = 12\ntest_batch_size = 12 \nloss_init = 1.0\nloss_dr = 0.7\nloss_de = 2\nlog_freq = 1\ntest_freq = 1\nkey = \"\"\n\nn_output = train_y.shape[1]\n\n\n\nemb_model = EmbeddingModel(n_models, max_ntree_per_split, embsize, maxleaf+1, n_output,\n                           group_average, task=task).to(device)\n\nopt = AdamW(emb_model.parameters(), lr=emb_lr, weight_decay=l2_reg)\ntree_outputs = np.asarray(tree_outputs).reshape((n_models, leaf_preds.shape[0])).transpose((1,0))\n\nTrainWithLog(loss_dr, loss_init, loss_de, log_freq, test_freq, task, test_batch_size,\n             leaf_preds, train_y, tree_outputs,\n                test_leaf_preds, test_y, emb_model, opt,\n                emb_epoch, batch_size, n_output, key+\"emb-\")\n\n\noutput_w = emb_model.bout.weight.data.cpu().numpy().reshape(n_models*embsize, n_output)\noutput_b = np.array(emb_model.bout.bias.data.cpu().numpy().sum())\ntrain_embs = GetEmbPred(emb_model, emb_model.lastlayer, leaf_preds, test_batch_size)\ndel tree_outputs, leaf_preds, test_leaf_preds\ngc.collect();\n",
   "outputs": [
    {
     "name": "stdout",
     "text": "emb-Epoch-000     1 Batches, Step      1, Training Loss:  7.354431 (AllAvg  7.354431)\nemb-Evaluate Result:\nEpoch-000     1 Batches, Step      1, Testing Loss:  3.300938, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-000     2 Batches, Step      2, Training Loss:  3.071912 (AllAvg  5.213171)\nemb-Evaluate Result:\nEpoch-000     2 Batches, Step      2, Testing Loss:  3.293565, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-000     3 Batches, Step      3, Training Loss:  3.969466 (AllAvg  4.798603)\nemb-Evaluate Result:\nEpoch-000     3 Batches, Step      3, Testing Loss:  3.355927, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-000     4 Batches, Step      4, Training Loss:  4.272389 (AllAvg  4.667049)\nemb-Evaluate Result:\nEpoch-000     4 Batches, Step      4, Testing Loss:  3.382929, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-000     5 Batches, Step      5, Training Loss:  5.767387 (AllAvg  4.887117)\nemb-Evaluate Result:\nEpoch-000     5 Batches, Step      5, Testing Loss:  3.384311, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-000     6 Batches, Step      6, Training Loss:  5.441872 (AllAvg  4.979576)\nemb-Evaluate Result:\nEpoch-000     6 Batches, Step      6, Testing Loss:  3.442703, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-000     7 Batches, Step      7, Training Loss:  0.982197 (AllAvg  4.408522)\nemb-Evaluate Result:\nEpoch-000     7 Batches, Step      7, Testing Loss:  3.437969, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-000     8 Batches, Step      8, Training Loss:  6.394997 (AllAvg  4.656831)\nemb-Evaluate Result:\nEpoch-000     8 Batches, Step      8, Testing Loss:  3.408447, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-000     9 Batches, Step      9, Training Loss: 17.605448 (AllAvg  5.174776)\nemb-Evaluate Result:\nEpoch-000     9 Batches, Step      9, Testing Loss:  3.515647, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 3.2935650539398194\n####################################################################################\nemb-Epoch-001     1 Batches, Step     10, Training Loss:  4.687627 (AllAvg  4.687627)\nemb-Evaluate Result:\nEpoch-001     1 Batches, Step     10, Testing Loss:  3.524631, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-001     2 Batches, Step     11, Training Loss:  3.760808 (AllAvg  4.224218)\nemb-Evaluate Result:\nEpoch-001     2 Batches, Step     11, Testing Loss:  3.499694, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-001     3 Batches, Step     12, Training Loss:  3.039086 (AllAvg  3.829174)\nemb-Evaluate Result:\nEpoch-001     3 Batches, Step     12, Testing Loss:  3.569342, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-001     4 Batches, Step     13, Training Loss:  4.860049 (AllAvg  4.086893)\nemb-Evaluate Result:\nEpoch-001     4 Batches, Step     13, Testing Loss:  3.586215, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-001     5 Batches, Step     14, Training Loss:  4.749975 (AllAvg  4.219509)\nemb-Evaluate Result:\nEpoch-001     5 Batches, Step     14, Testing Loss:  3.515219, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-001     6 Batches, Step     15, Training Loss:  4.520470 (AllAvg  4.269669)\nemb-Evaluate Result:\nEpoch-001     6 Batches, Step     15, Testing Loss:  3.558439, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-001     7 Batches, Step     16, Training Loss:  3.563529 (AllAvg  4.168792)\nemb-Evaluate Result:\nEpoch-001     7 Batches, Step     16, Testing Loss:  3.504626, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-001     8 Batches, Step     17, Training Loss:  7.480133 (AllAvg  4.582710)\nemb-Evaluate Result:\nEpoch-001     8 Batches, Step     17, Testing Loss:  3.452472, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-001     9 Batches, Step     18, Training Loss: 13.081884 (AllAvg  4.922677)\nemb-Evaluate Result:\nEpoch-001     9 Batches, Step     18, Testing Loss:  3.500860, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 3.2935650539398194\n####################################################################################\nemb-Epoch-002     1 Batches, Step     19, Training Loss:  3.845772 (AllAvg  3.845772)\nemb-Evaluate Result:\nEpoch-002     1 Batches, Step     19, Testing Loss:  3.426239, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-002     2 Batches, Step     20, Training Loss:  7.933588 (AllAvg  5.889680)\nemb-Evaluate Result:\nEpoch-002     2 Batches, Step     20, Testing Loss:  3.384868, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-002     3 Batches, Step     21, Training Loss:  5.312322 (AllAvg  5.697227)\nemb-Evaluate Result:\nEpoch-002     3 Batches, Step     21, Testing Loss:  3.494605, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-002     4 Batches, Step     22, Training Loss:  4.147663 (AllAvg  5.309836)\nemb-Evaluate Result:\nEpoch-002     4 Batches, Step     22, Testing Loss:  3.406167, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-002     5 Batches, Step     23, Training Loss:  4.626566 (AllAvg  5.173182)\nemb-Evaluate Result:\nEpoch-002     5 Batches, Step     23, Testing Loss:  3.474935, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-002     6 Batches, Step     24, Training Loss:  3.228472 (AllAvg  4.849064)\nemb-Evaluate Result:\nEpoch-002     6 Batches, Step     24, Testing Loss:  3.494696, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-002     7 Batches, Step     25, Training Loss:  7.823696 (AllAvg  5.274011)\nemb-Evaluate Result:\nEpoch-002     7 Batches, Step     25, Testing Loss:  3.505036, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-002     8 Batches, Step     26, Training Loss:  4.040615 (AllAvg  5.119837)\nemb-Evaluate Result:\nEpoch-002     8 Batches, Step     26, Testing Loss:  3.556610, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Epoch-002     9 Batches, Step     27, Training Loss:  1.190546 (AllAvg  4.962665)\nemb-Evaluate Result:\nEpoch-002     9 Batches, Step     27, Testing Loss:  3.354475, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 3.2935650539398194\n####################################################################################\nFinal Best Metric: 3.2935650539398194\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5c0513cc",
    "execution_start": 1644069476363,
    "execution_millis": 0,
    "cell_id": "00016-9c9fdea4-d787-4594-9b49-92749b1a6b7d",
    "deepnote_cell_type": "code"
   },
   "source": "with open('train_embs_iris_2.pickle', 'wb') as f:\n    # Pickle using the highest protocol available.\n    pickle.dump(train_embs, f, pickle.HIGHEST_PROTOCOL)\n    \nwith open('output_w_iris_2.pickle', 'wb') as f:\n    # Pickle using the highest protocol available.\n    pickle.dump(output_w, f, pickle.HIGHEST_PROTOCOL)\n    \nwith open('output_b_iris_2.pickle', 'wb') as f:\n    # Pickle using the highest protocol available.\n    pickle.dump(output_b, f, pickle.HIGHEST_PROTOCOL)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=de072003-a9db-4342-8067-19a4b45feff1' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "e43b6c38-946b-466a-9cac-755a55cd2797",
  "deepnote_execution_queue": []
 }
}