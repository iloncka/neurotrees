{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "47b5d1f8",
    "execution_start": 1644227270790,
    "execution_millis": 802,
    "cell_id": "ed9f09f3-8ceb-41ac-befd-10247f1f7e49",
    "deepnote_cell_type": "code"
   },
   "source": "import wandb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "aa697592",
    "execution_start": 1644227271598,
    "execution_millis": 990,
    "cell_id": "00001-b7cbbe54-4483-4e98-beea-39fe7942041f",
    "deepnote_cell_type": "code"
   },
   "source": "import argparse, os, logging, random, time\nimport numpy as np\nimport math\nimport time\nimport scipy.sparse\nimport lightgbm as lgb\nimport data_helpers as dh",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e9764fd6",
    "execution_start": 1644227272595,
    "execution_millis": 1329,
    "cell_id": "00002-329f4964-bbb8-4040-9eca-3b19c20ad10d",
    "deepnote_cell_type": "code"
   },
   "source": "import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom sklearn.utils.extmath import softmax\n\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\nfrom torch.optim import Optimizer, AdamW, SGD\n\nimport gc",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "58de1d54",
    "execution_start": 1644227273941,
    "execution_millis": 16,
    "deepnote_output_heights": [
     21
    ],
    "cell_id": "00003-c7ed462a-8fbf-4c33-91c3-7bdb7a0470bb",
    "deepnote_cell_type": "code"
   },
   "source": "torch.__version__",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 4,
     "data": {
      "text/plain": "'1.10.0+cu102'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "576d90f9",
    "execution_start": 1644227273954,
    "execution_millis": 22,
    "deepnote_output_heights": [
     21
    ],
    "cell_id": "00004-759abf02-197d-4a77-8fbd-3aa13035b1bd",
    "deepnote_cell_type": "code"
   },
   "source": "torchvision.__version__",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 5,
     "data": {
      "text/plain": "'0.11.1+cu102'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "280b5616",
    "execution_start": 1644227273985,
    "execution_millis": 10,
    "cell_id": "00005-a69c80a2-a2d5-461d-b59b-2dfc8433cba0",
    "deepnote_cell_type": "code"
   },
   "source": "import pdb\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif torch.cuda.is_available():\n    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n    type_prefix = torch.cuda\nelse:\n    type_prefix = torch",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "47cd33c8",
    "execution_start": 1644227274042,
    "execution_millis": 0,
    "is_code_hidden": true,
    "cell_id": "00006-41ec514a-becc-4033-b6fa-417f1bfffeb3",
    "deepnote_cell_type": "code"
   },
   "source": "def one_hot(y, numslot, mask=None):\n    y_tensor = y.type(type_prefix.LongTensor).reshape(-1, 1)\n    y_one_hot = torch.zeros(y_tensor.size()[0], numslot, device=device, dtype=torch.float32, requires_grad=False).scatter_(1, y_tensor, 1)\n    if mask is not None:\n        y_one_hot = y_one_hot * mask\n    y_one_hot = y_one_hot.reshape(y.shape[0], -1)\n    return y_one_hot",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "91eaa51",
    "execution_start": 1644227274042,
    "execution_millis": 1,
    "is_code_hidden": false,
    "cell_id": "00007-7d73c01b-ac4a-4237-b2f5-437c7f493545",
    "deepnote_cell_type": "code"
   },
   "source": "class BatchDense(nn.Module):\n    def __init__(self, batch, in_features, out_features, bias_init=None):\n        super(BatchDense, self).__init__()\n        self.batch = batch\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.Tensor(batch, in_features, out_features))\n        self.bias = Parameter(torch.Tensor(batch, 1, out_features))\n        self.reset_parameters(bias_init)\n    def reset_parameters(self, bias_init=None):\n        stdv = math.sqrt(6.0 /(self.in_features + self.out_features))\n        self.weight.data.uniform_(-stdv, stdv)\n        if bias_init is not None:\n            # pdb.set_trace()\n            self.bias.data = torch.from_numpy(np.array(bias_init))\n            \n        else:\n            self.bias.data.fill_(0)\n    def forward(self, x):\n        size = x.size()\n        # Todo: avoid the swap axis\n        x = x.view(x.size(0), self.batch, -1)\n        out = x.transpose(0, 1).contiguous()\n        out = torch.baddbmm(self.bias, out, self.weight)\n        out = out.transpose(0, 1).contiguous()\n        out = out.view(x.size(0), -1)\n        return out",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "18e69f81",
    "execution_start": 1644227274077,
    "execution_millis": 1,
    "is_code_hidden": false,
    "cell_id": "00008-39dd8409-2899-4104-8bb1-91f7b85794d8",
    "deepnote_cell_type": "code"
   },
   "source": "class EmbeddingModel(nn.Module):\n    def __init__(self, n_models, max_ntree_per_split, embsize, maxleaf, n_output, out_bias=None, task='regression'):\n        super(EmbeddingModel, self).__init__()\n        self.task = task\n        self.n_models = n_models\n        self.maxleaf = maxleaf\n        self.fcs = nn.ModuleList()\n        self.max_ntree_per_split = max_ntree_per_split\n\n        self.embed_w = Parameter(torch.Tensor(n_models, max_ntree_per_split*maxleaf, embsize))\n        # torch.nn.init.xavier_normal(self.embed_w)\n        stdv = math.sqrt(1.0 /(max_ntree_per_split))\n        self.embed_w.data.normal_(0,stdv) # .uniform_(-stdv, stdv)\n        \n        self.bout = BatchDense(n_models, embsize, 1, out_bias)\n        self.bn = nn.BatchNorm1d(embsize * n_models)\n        self.tanh = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n        # self.output_fc = Dense(n_models * embsize, n_output)\n        self.dropout = torch.nn.Dropout()\n        if task == 'regression':\n            self.criterion = nn.MSELoss()\n        else:\n            self.criterion = nn.BCELoss()\n\n    def batchmul(self, x, models, embed_w, length):\n        out = one_hot(x, length)\n        out = out.view(x.size(0), models, -1)\n        out = out.transpose(0, 1).contiguous()\n        out = torch.bmm(out, embed_w)\n        out = out.transpose(0, 1).contiguous()\n        out = out.view(x.size(0), -1)\n        return out\n        \n    def lastlayer(self, x):\n        out = self.batchmul(x, self.n_models, self.embed_w, self.maxleaf)\n        out = self.bn(out)\n        # out = self.tanh(out)\n        # out = out.view(x.size(0), self.n_models, -1)\n        return out\n    def forward(self, x):\n        out = self.lastlayer(x)\n        out = self.dropout(out)\n        out = out.view(x.size(0), self.n_models, -1)\n        out = self.bout(out)\n        # out = self.output_fc(out)\n        sum_out = torch.sum(out,-1,True)\n        if self.task != 'regression':\n            return self.sigmoid(sum_out), out\n        return sum_out, out\n    \n    def joint_loss(self, out, target, out_inner, target_inner, *args):\n        return nn.MSELoss()(out_inner, target_inner)\n\n    def true_loss(self, out, target):\n        return self.criterion(out, target)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "575bb924",
    "execution_start": 1644227274078,
    "execution_millis": 37,
    "is_code_hidden": false,
    "cell_id": "00009-76e91e16-d7b7-4d20-bc47-9eb86185b2b1",
    "deepnote_cell_type": "code"
   },
   "source": "def eval_metrics(task, true, pred):\n    if task == 'binary':\n        logloss = sklearn.metrics.log_loss(true.astype(np.float64), pred.astype(np.float64))\n        auc = sklearn.metrics.roc_auc_score(true, pred)\n        # error = 1-sklearn.metrics.accuracy_score(true,(pred+0.5).astype(np.int32))\n        return (logloss, auc)#, error)\n    else:\n        mseloss = sklearn.metrics.mean_squared_error(true, pred)\n        return mseloss\n\ndef EvalTestset(test_x, test_y, model, test_batch_size, test_x_opt=None):\n    test_len = test_x.shape[0]\n    test_num_batch = math.ceil(test_len / test_batch_size)\n    sum_loss = 0.0\n    y_preds = []\n    model.eval()\n    with torch.no_grad():\n        for jdx in range(test_num_batch):\n            tst_st = jdx * test_batch_size\n            tst_ed = min(test_len, tst_st + test_batch_size)\n            inputs = torch.from_numpy(test_x[tst_st:tst_ed].astype(np.float32)).to(device)\n            if test_x_opt is not None:\n                inputs_opt = torch.from_numpy(test_x_opt[tst_st:tst_ed].astype(np.float32)).to(device)\n                outputs = model(inputs, inputs_opt)\n            else:\n                outputs = model(inputs)\n            targets = torch.from_numpy(test_y[tst_st:tst_ed]).to(device)\n            if isinstance(outputs, tuple):\n                outputs = outputs[0]\n            y_preds.append(outputs)\n            loss_tst = model.true_loss(outputs, targets).item()            \n            sum_loss += (tst_ed - tst_st) * loss_tst\n    return sum_loss / test_len, np.concatenate(y_preds, 0)\n\ndef TrainWithLog(loss_dr, loss_init, loss_de, log_freq, test_freq, task, test_batch_size,                \n                train_x, train_y, \n                 train_y_opt, test_x, test_y, model, opt,\n                 epoch, batch_size, n_output, key=\"\",\n                 train_x_opt=None, test_x_opt=None):\n    # trn_writer = tf.summary.FileWriter(summaryPath+plot_title+key+\"_output/train\")\n    # tst_writer = tf.summary.FileWriter(summaryPath+plot_title+key+\"_output/test\")\n    if isinstance(test_x, scipy.sparse.csr_matrix):\n        test_x = test_x.todense()\n    train_len = train_x.shape[0]\n    global_iter = 0\n    trn_batch_size = batch_size\n    train_num_batch = math.ceil(train_len / trn_batch_size)\n    total_iterations = epoch * train_num_batch\n    start_time = time.time()\n    total_time = 0.0\n    min_loss = float(\"Inf\")\n    # min_error = float(\"Inf\")\n    max_auc = 0.0\n    for epoch in range(epoch):\n        shuffled_indices = np.random.permutation(np.arange(train_x.shape[0]))\n        Loss_trn_epoch = 0.0\n        Loss_trn_log = 0.0\n        log_st = 0\n        for local_iter in range(train_num_batch):\n            trn_st = local_iter * trn_batch_size\n            trn_ed = min(train_len, trn_st + trn_batch_size)\n            batch_trn_x = train_x[shuffled_indices[trn_st:trn_ed]]\n            if isinstance(batch_trn_x, scipy.sparse.csr_matrix):\n                batch_trn_x = batch_trn_x.todense()\n            inputs = torch.from_numpy(batch_trn_x.astype(np.float32)).to(device)\n            targets = torch.from_numpy(train_y[shuffled_indices[trn_st:trn_ed],:]).to(device)\n            model.train()\n            if train_x_opt is not None:\n                inputs_opt = torch.from_numpy(train_x_opt[shuffled_indices[trn_st:trn_ed]].astype(np.float32)).to(device)\n                outputs = model(inputs, inputs_opt)\n            else:\n                outputs = model(inputs)\n            opt.zero_grad()\n            if isinstance(outputs, tuple) and train_y_opt is not None:\n                # targets_inner = torch.from_numpy(s_train_y_opt[trn_st:trn_ed,:]).to(device)\n                targets_inner = torch.from_numpy(train_y_opt[shuffled_indices[trn_st:trn_ed],:]).to(device)\n                loss_ratio = loss_init * max(0.3,loss_dr ** (epoch // loss_de))#max(0.5, args.loss_dr ** (epoch // args.loss_de))\n                if len(outputs) == 3:\n                    loss_val = model.joint_loss(outputs[0], targets, outputs[1], targets_inner, loss_ratio, outputs[2])\n                else:\n                    loss_val = model.joint_loss(outputs[0], targets, outputs[1], targets_inner, loss_ratio)\n                loss_val.backward()\n                loss_val = model.true_loss(outputs[0], targets)\n            elif isinstance(outputs, tuple):\n                loss_val = model.true_loss(outputs[0], targets)\n                loss_val.backward()\n            else:\n                loss_val = model.true_loss(outputs, targets)\n                loss_val.backward()\n            opt.step()\n            loss_val = loss_val.item()\n            wandb.log({\"batch loss\":loss_val})\n            global_iter += 1\n            Loss_trn_epoch += (trn_ed - trn_st) * loss_val\n            Loss_trn_log += (trn_ed - trn_st) * loss_val\n            if global_iter % log_freq == 0:\n                print(key+\"Epoch-{:0>3d} {:>5d} Batches, Step {:>6d}, Training Loss: {:>9.6f} (AllAvg {:>9.6f})\"\n                            .format(epoch, local_iter + 1, global_iter, Loss_trn_log/(trn_ed-log_st), Loss_trn_epoch/trn_ed))\n                \n                # trn_summ = tf.Summary()\n                # trn_summ.value.add(tag=args.data+ \"/Train/Loss\", simple_value = Loss_trn_log/(trn_ed-log_st))\n                # trn_writer.add_summary(trn_summ, global_iter)\n                log_st = trn_ed\n                Loss_trn_log = 0.0\n            if global_iter % test_freq == 0 or local_iter == train_num_batch - 1:\n                if model == 'deepgbm' or model == 'd1':\n                    try:\n                        print('Alpha: '+str(model.alpha))\n                        print('Beta: '+str(model.beta))\n                    except:\n                        pass\n                # tst_summ = tf.Summary()\n                torch.cuda.empty_cache()\n                test_loss, pred_y = EvalTestset(test_x, test_y, model, test_batch_size, test_x_opt)\n                wandb.log({\"loss\":test_loss})\n                current_used_time = time.time() - start_time\n                start_time = time.time()\n                wandb.log({\"createdAt\":start_time})\n                total_time += current_used_time\n                remaining_time = (total_iterations - (global_iter) ) * (total_time / (global_iter))\n                if task == 'binary':\n                    metrics = eval_metrics(task, test_y, pred_y)\n                    _, test_auc = metrics\n                    wandb.log({\"test batch auc\":test_auc})\n                    # min_error = min(min_error, test_error)\n                    max_auc = max(max_auc, test_auc)\n                    wandb.log({\"test max auc\":max_auc})\n                    # tst_summ.value.add(tag=args.data+\"/Test/Eval/Error\", simple_value = test_error)\n                    # tst_summ.value.add(tag=args.data+\"/Test/Eval/AUC\", simple_value = test_auc)\n                    # tst_summ.value.add(tag=args.data+\"/Test/Eval/Min_Error\", simple_value = min_error)\n                    # tst_summ.value.add(tag=args.data+\"/Test/Eval/Max_AUC\", simple_value = max_auc)\n                    print(key+\"Evaluate Result:\\nEpoch-{:0>3d} {:>5d} Batches, Step {:>6d}, Testing Loss: {:>9.6f}, Testing AUC: {:8.6f}, Used Time: {:>5.1f}m, Remaining Time: {:5.1f}m\"\n                            .format(epoch, local_iter + 1, global_iter, test_loss, test_auc, total_time/60.0, remaining_time/60.0))\n                else:\n                    print(key+\"Evaluate Result:\\nEpoch-{:0>3d} {:>5d} Batches, Step {:>6d}, Testing Loss: {:>9.6f}, Used Time: {:>5.1f}m, Remaining Time: {:5.1f}m\"\n                            .format(epoch, local_iter + 1, global_iter, test_loss, total_time/60.0, remaining_time/60.0))\n                min_loss = min(min_loss, test_loss)\n                wandb.log({\"test min loss\": min_loss})\n                # tst_summ.value.add(tag=args.data+\"/Test/Loss\", simple_value = test_loss)\n                # tst_summ.value.add(tag=args.data+\"/Test/Min_Loss\", simple_value = min_loss)\n                print(\"-------------------------------------------------------------------------------\")\n                # tst_writer.add_summary(tst_summ, global_iter)\n                # tst_writer.flush()\n        print(\"Best Metric: %s\"%(str(max_auc) if task=='binary' else str(min_loss)))\n        print(\"####################################################################################\")\n    print(\"Final Best Metric: %s\"%(str(max_auc) if task=='binary' else str(min_loss)))\n    return min_loss        \n\ndef GetEmbPred(model, fun, X, test_batch_size):\n    model.eval()\n    tst_len = X.shape[0]\n    test_num_batch = math.ceil(tst_len / test_batch_size)\n    y_preds = []\n    with torch.no_grad():\n        for jdx in range(test_num_batch):\n            tst_st = jdx * test_batch_size\n            tst_ed = min(tst_len, tst_st + test_batch_size)\n            inputs = torch.from_numpy(X[tst_st:tst_ed]).to(device)\n            t_preds = fun(inputs).data.cpu().numpy()\n            y_preds.append(t_preds)\n        y_preds = np.concatenate(y_preds, 0)\n    return y_preds\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7ddb1838",
    "execution_start": 1644227274163,
    "execution_millis": 0,
    "cell_id": "00010-088feb1f-12c3-4197-ae04-b17935257e63",
    "deepnote_cell_type": "code"
   },
   "source": "HOME_DIR = os.getcwd()\nDATA_DIR = os.path.join(HOME_DIR, 'data')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "15d6e06e",
    "execution_start": 1644227274164,
    "execution_millis": 2,
    "cell_id": "00011-ed500c8e-e6a1-4ae8-8458-1da601e0bf99",
    "deepnote_cell_type": "code"
   },
   "source": "num_data = dh.load_data('/work/neurotrees/articles code reproduction/DeepGBM/data/data_offline_num')",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "2022-02-07 09:47:54,148 [INFO] data loaded.\n train_x shape: (3918, 12). train_y shape: (3918, 1).\n test_x shape: (980, 12). test_y shape: (980, 1).\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "14a37530",
    "execution_start": 1644227274206,
    "execution_millis": 0,
    "cell_id": "00012-a635c196-8aa9-4722-a25d-66a44ebb5d22",
    "deepnote_cell_type": "code"
   },
   "source": "train_x, train_y, test_x, test_y = num_data",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "22f94691",
    "execution_start": 1644227274207,
    "execution_millis": 46947086,
    "cell_id": "00013-66fcd861-733d-4053-8d46-7bb86a005f1d",
    "deepnote_cell_type": "code"
   },
   "source": "PATH_TO_PICKLE = '/work/neurotrees/experiments/DeepGBM-decomposition/wine-dataset'",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b6e57a1c",
    "execution_start": 1644227274250,
    "execution_millis": 46947086,
    "cell_id": "00014-bd8dcd4b-ee33-4468-afac-b5423539f874",
    "deepnote_cell_type": "code"
   },
   "source": "import pickle    ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f2cbfc7d",
    "execution_start": 1644227274250,
    "execution_millis": 46947085,
    "cell_id": "00015-4e4bcac7-ab7c-400b-a2ad-f820d3bb11dd",
    "deepnote_cell_type": "code"
   },
   "source": "sweep_config = {\n    'method': 'random', #grid, random\n    'metric': {\n      'name': 'loss',\n      'goal': 'minimize'   \n    },\n    'parameters': {\n        'emb_epoch': {\n            'values': [2, 5, 10]\n        },\n        'batch_size': {\n            'values': [256, 128, 64, 32]\n        },\n        \n        'emb_lr': {\n            'values': [1e-2, 1e-3, 1e-4, 3e-4, 3e-5, 1e-5]\n        },\n        \n        'optimizer': {\n            'values': ['adamW', 'sgd']\n        },\n    }\n}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ec1dc211",
    "execution_start": 1644227274251,
    "execution_millis": 998,
    "cell_id": "00016-6a334d90-6907-45c0-874f-573d47e3f74f",
    "deepnote_cell_type": "code"
   },
   "source": "sweep_id = wandb.sweep(sweep_config, project=\"deepgbm-wandb\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "2022-02-07 09:47:54,553 [INFO] Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nCreate sweep with ID: r48kqhtn\nSweep URL: https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d10240fd",
    "execution_start": 1644227275317,
    "execution_millis": 1,
    "deepnote_output_heights": [
     611
    ],
    "cell_id": "00017-c5d31b22-a32e-4b51-a786-10efd85ece4d",
    "deepnote_cell_type": "code"
   },
   "source": "def train():\n   # Default values for hyper-parameters we're going to sweep over\n   with open(os.path.join(PATH_TO_PICKLE,'n_models_wine_100.pickle'), 'rb') as f:\n    # Pickle using the highest protocol available.\n    n_models = pickle.load(f)\n    \n   with open(os.path.join(PATH_TO_PICKLE,'max_ntree_per_split_wine_100.pickle'), 'rb') as f:\n      # Pickle using the highest protocol available.\n      max_ntree_per_split = pickle.load(f)\n      \n   with open(os.path.join(PATH_TO_PICKLE,'group_average_wine_100.pickle'), 'rb') as f:\n      # Pickle using the highest protocol available.\n      group_average = pickle.load(f)\n\n   with open(os.path.join(PATH_TO_PICKLE,'leaf_preds_wine_100.pickle'), 'rb') as f:\n      # Pickle using the highest protocol available.\n      leaf_preds = pickle.load(f)\n      \n   with open(os.path.join(PATH_TO_PICKLE,'test_leaf_preds_wine_100.pickle'), 'rb') as f:\n      # Pickle using the highest protocol available.\n      test_leaf_preds = pickle.load(f)\n      \n   with open(os.path.join(PATH_TO_PICKLE,'tree_outputs_wine_100.pickle'), 'rb') as f:\n      # Pickle using the highest protocol available.\n      tree_outputs = pickle.load(f) \n\n   config_defaults = dict(\n      \n      n_models = n_models,\n      max_ntree_per_split = max_ntree_per_split,\n      group_average = group_average,    \n      embsize = 20,\n      maxleaf = 64,\n      task = \"regression\",\n      l2_reg = 1e-6,\n      emb_lr = 1e-3,\n      emb_epoch = 2,\n      batch_size = 512,\n      test_batch_size = 100,\n      loss_init = 1.0,\n      loss_dr = 0.7,\n      loss_de = 2,\n      log_freq = 500,\n      test_freq = 300,\n      key = \"\",\n      n_output = train_y.shape[1]\n      )\n\n\n   # Initialize a new wandb run\n   wandb.init(config=config_defaults)\n    \n   # Config is a variable that holds and saves hyperparameters and inputs\n   config = wandb.config\n    \n \n   emb_model = EmbeddingModel(config.n_models, config.max_ntree_per_split, \n                              config.embsize,\n                              config.maxleaf+1, config.n_output,\n                              config.group_average, task=config.task).float().to(device)\n   if config.optimizer=='sgd':\n         opt = SGD(emb_model.parameters(),lr=config.emb_lr, momentum=0.9)\n   elif config.optimizer=='adamW':\n         opt = AdamW(emb_model.parameters(),lr=config.emb_lr, weight_decay=config.l2_reg)\n\n   tree_outputs = np.asarray(tree_outputs).reshape((config.n_models, \n                  leaf_preds.shape[0])).transpose((1,0))\n\n   TrainWithLog(config.loss_dr, config.loss_init, config.loss_de, config.log_freq, \n               config.test_freq, \n               config.task, config.test_batch_size,\n               leaf_preds, train_y, tree_outputs,\n               test_leaf_preds, test_y, emb_model, opt,\n               config.emb_epoch, config.batch_size, config.n_output, config.key+\"emb-\")\n\n\n   output_w = emb_model.bout.weight.data.cpu().numpy().reshape(config.n_models*config.embsize, config.n_output)\n   output_b = np.array(emb_model.bout.bias.data.cpu().numpy().sum())\n   train_embs = GetEmbPred(emb_model, emb_model.lastlayer, leaf_preds,\n                      config.test_batch_size)\n   del tree_outputs, leaf_preds, test_leaf_preds\n   gc.collect();\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a7c22e3f",
    "execution_start": 1644227275318,
    "execution_millis": 720065,
    "deepnote_output_heights": [
     null,
     40,
     null,
     42,
     408,
     null,
     40,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     40,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     40,
     null,
     40,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     40,
     null,
     40,
     409,
     null,
     42,
     null,
     42,
     408,
     null,
     42,
     null,
     42,
     408,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     40,
     null,
     42,
     409,
     null,
     42,
     null,
     40,
     409,
     null,
     42,
     null,
     42,
     408,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     408,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     40,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     408,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     42,
     null,
     42,
     409,
     null,
     40,
     null,
     40,
     408,
     null,
     40,
     null,
     40,
     408,
     null,
     40,
     null,
     40,
     408,
     null,
     40,
     null,
     40,
     408,
     null,
     40,
     null,
     40,
     408,
     null,
     40,
     null,
     40,
     408,
     null,
     40,
     null,
     40,
     408,
     null,
     40,
     null,
     40,
     408,
     null,
     40,
     null,
     40,
     408
    ],
    "cell_id": "00019-81126e74-8511-457b-9f40-984189ef75e4",
    "deepnote_cell_type": "code"
   },
   "source": "wandb.agent(sweep_id, train)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "2022-02-07 09:47:55,285 [INFO] Starting sweep agent: entity=None, project=None, count=None\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k3i55uww with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n2022-02-07 09:47:55,631 [INFO] Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miloncka\u001b[0m (use `wandb login --relogin` to force relogin)\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/k3i55uww\" target=\"_blank\">absurd-sweep-1</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 39.182088, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 39.182087567387796\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 36.072282, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 36.072281584447744\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    31 Batches, Step     93, Testing Loss: 34.390575, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 34.39057494182976\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    31 Batches, Step    124, Testing Loss: 33.252328, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 33.25232821094747\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    31 Batches, Step    155, Testing Loss: 31.386464, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 31.386463593463507\n####################################################################################\nemb-Evaluate Result:\nEpoch-005    31 Batches, Step    186, Testing Loss: 28.955318, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 28.95531771134357\n####################################################################################\nemb-Evaluate Result:\nEpoch-006    31 Batches, Step    217, Testing Loss: 27.962015, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 27.96201468487175\n####################################################################################\nemb-Evaluate Result:\nEpoch-007    31 Batches, Step    248, Testing Loss: 26.720646, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 26.720645904541016\n####################################################################################\nemb-Evaluate Result:\nEpoch-008    31 Batches, Step    279, Testing Loss: 25.310935, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 25.31093453387825\n####################################################################################\nemb-Evaluate Result:\nEpoch-009    21 Batches, Step    300, Testing Loss: 24.364462, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-009    31 Batches, Step    310, Testing Loss: 24.309035, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 24.309034775714483\n####################################################################################\nFinal Best Metric: 24.309034775714483\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 632... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆▇█▇▆▆▅▅▆▇▅▅▂▄▃▄▄▄▅▃▄▃▂▃▂▃▄▃▃▃▃▂▂▂▁▁▂▂▂▂</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▇▆▅▄▃▃▂▁▁▁</td></tr><tr><td>test min loss</td><td>█▇▆▅▄▃▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>26.07387</td></tr><tr><td>createdAt</td><td>1644227285.3567</td></tr><tr><td>loss</td><td>24.30903</td></tr><tr><td>test min loss</td><td>24.30903</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">absurd-sweep-1</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/k3i55uww\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/k3i55uww</a><br/>\nFind logs at: <code>./wandb/run-20220207_094755-k3i55uww/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0zjlqb9x with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/0zjlqb9x\" target=\"_blank\">different-sweep-2</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 13.489278, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 13.48927776180968\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss:  6.690345, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 6.6903451900092925\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    54 Batches, Step    300, Testing Loss:  4.709650, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-002   123 Batches, Step    369, Testing Loss:  2.831117, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 2.831117260212801\n####################################################################################\nemb-Evaluate Result:\nEpoch-003   123 Batches, Step    492, Testing Loss:  1.719265, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 1.7192652055195399\n####################################################################################\nemb-Epoch-004     8 Batches, Step    500, Training Loss:  3.139378 (AllAvg  3.139378)\nemb-Evaluate Result:\nEpoch-004   108 Batches, Step    600, Testing Loss:  1.237801, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004   123 Batches, Step    615, Testing Loss:  0.991899, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.991898741040911\n####################################################################################\nFinal Best Metric: 0.991898741040911\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 671... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▅█▄▅▅▅▄▃▂▂▃▃▂▂▂▂▂▂▁▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▃▂▁▁▁</td></tr><tr><td>test min loss</td><td>█▄▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>1.90597</td></tr><tr><td>createdAt</td><td>1644227299.69356</td></tr><tr><td>loss</td><td>0.9919</td></tr><tr><td>test min loss</td><td>0.9919</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">different-sweep-2</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/0zjlqb9x\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/0zjlqb9x</a><br/>\nFind logs at: <code>./wandb/run-20220207_094811-0zjlqb9x/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c8k00rh1 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/c8k00rh1\" target=\"_blank\">faithful-sweep-3</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss:  0.547711, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.5477105591978345\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss:  0.143716, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.14371613686790272\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    62 Batches, Step    186, Testing Loss:  0.121142, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.12114209058333417\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    62 Batches, Step    248, Testing Loss:  0.104858, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.10485805221358124\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    52 Batches, Step    300, Testing Loss:  0.080925, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004    62 Batches, Step    310, Testing Loss:  0.067416, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.06741646183084468\n####################################################################################\nemb-Evaluate Result:\nEpoch-005    62 Batches, Step    372, Testing Loss:  0.061166, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.06116599277878294\n####################################################################################\nemb-Evaluate Result:\nEpoch-006    62 Batches, Step    434, Testing Loss:  0.065349, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.06116599277878294\n####################################################################################\nemb-Evaluate Result:\nEpoch-007    62 Batches, Step    496, Testing Loss:  0.057764, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.057763839938810894\n####################################################################################\nemb-Epoch-008     4 Batches, Step    500, Training Loss:  0.110766 (AllAvg  0.110766)\nemb-Evaluate Result:\nEpoch-008    62 Batches, Step    558, Testing Loss:  0.046794, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.04679357290876155\n####################################################################################\nemb-Evaluate Result:\nEpoch-009    42 Batches, Step    600, Testing Loss:  0.052006, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-009    62 Batches, Step    620, Testing Loss:  0.040633, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.04063256183753208\n####################################################################################\nFinal Best Metric: 0.04063256183753208\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 709... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>test min loss</td><td>█▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.10759</td></tr><tr><td>createdAt</td><td>1644227317.58068</td></tr><tr><td>loss</td><td>0.04063</td></tr><tr><td>test min loss</td><td>0.04063</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">faithful-sweep-3</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/c8k00rh1\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/c8k00rh1</a><br/>\nFind logs at: <code>./wandb/run-20220207_094826-c8k00rh1/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vgzgk7ju with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/vgzgk7ju\" target=\"_blank\">olive-sweep-4</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    16 Batches, Step     16, Testing Loss: 33.695701, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 33.69570105416434\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    16 Batches, Step     32, Testing Loss: 27.528355, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 27.52835546221052\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    16 Batches, Step     48, Testing Loss: 23.360705, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 23.36070527835768\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    16 Batches, Step     64, Testing Loss: 19.705508, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 19.705507628771723\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    16 Batches, Step     80, Testing Loss: 17.297251, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 17.297250903382594\n####################################################################################\nFinal Best Metric: 17.297250903382594\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 747... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>██▇█▆▆▆▇▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▁▂▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▅▄▂▁</td></tr><tr><td>test min loss</td><td>█▅▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>22.35398</td></tr><tr><td>createdAt</td><td>1644227338.35088</td></tr><tr><td>loss</td><td>17.29725</td></tr><tr><td>test min loss</td><td>17.29725</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">olive-sweep-4</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/vgzgk7ju\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/vgzgk7ju</a><br/>\nFind logs at: <code>./wandb/run-20220207_094852-vgzgk7ju/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o8oocezm with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/o8oocezm\" target=\"_blank\">giddy-sweep-5</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    16 Batches, Step     16, Testing Loss: 11.265538, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 11.265537962621572\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    16 Batches, Step     32, Testing Loss:  3.558882, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 3.5588822851375657\n####################################################################################\nFinal Best Metric: 3.5588822851375657\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 785... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇█▇▆▇▆▅▅▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>3.23389</td></tr><tr><td>createdAt</td><td>1644227357.03465</td></tr><tr><td>loss</td><td>3.55888</td></tr><tr><td>test min loss</td><td>3.55888</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">giddy-sweep-5</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/o8oocezm\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/o8oocezm</a><br/>\nFind logs at: <code>./wandb/run-20220207_094912-o8oocezm/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nzqxixwy with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/nzqxixwy\" target=\"_blank\">feasible-sweep-6</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 28.224731, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 28.22473109498316\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 17.214028, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 17.21402816383206\n####################################################################################\nFinal Best Metric: 17.21402816383206\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 823... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆▆▆▇▆▅█▅▅▅▆▄▄▆▅▅▄▅▃▃▄▃▂▃▃▂▂▂▃▃▂▂▂▂▂▂▂▁▂▁</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>18.26738</td></tr><tr><td>createdAt</td><td>1644227367.36355</td></tr><tr><td>loss</td><td>17.21403</td></tr><tr><td>test min loss</td><td>17.21403</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">feasible-sweep-6</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/nzqxixwy\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/nzqxixwy</a><br/>\nFind logs at: <code>./wandb/run-20220207_094922-nzqxixwy/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kuia55cf with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/kuia55cf\" target=\"_blank\">celestial-sweep-7</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss:  8.966824, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 8.9668242590768\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss:  4.208235, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 4.208234519374614\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    54 Batches, Step    300, Testing Loss:  3.183000, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-002   123 Batches, Step    369, Testing Loss:  2.297452, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 2.2974516810203087\n####################################################################################\nemb-Evaluate Result:\nEpoch-003   123 Batches, Step    492, Testing Loss:  1.441442, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 1.4414424823254954\n####################################################################################\nemb-Epoch-004     8 Batches, Step    500, Training Loss:  1.524587 (AllAvg  1.524587)\nemb-Evaluate Result:\nEpoch-004   108 Batches, Step    600, Testing Loss:  1.015355, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004   123 Batches, Step    615, Testing Loss:  0.985385, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.9853847744513531\n####################################################################################\nFinal Best Metric: 0.9853847744513531\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 861... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇█▆▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▃▂▁▁▁</td></tr><tr><td>test min loss</td><td>█▄▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.83689</td></tr><tr><td>createdAt</td><td>1644227380.7463</td></tr><tr><td>loss</td><td>0.98538</td></tr><tr><td>test min loss</td><td>0.98538</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">celestial-sweep-7</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/kuia55cf\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/kuia55cf</a><br/>\nFind logs at: <code>./wandb/run-20220207_094932-kuia55cf/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m5c3n5pz with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/m5c3n5pz\" target=\"_blank\">fine-sweep-8</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 14.850186, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 14.85018607548305\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss:  6.046667, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 6.0466672936264345\n####################################################################################\nFinal Best Metric: 6.0466672936264345\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 899... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇▅▇█▇▆▅▅▄▄▃▆▃▃▃▃▄▃▂▂▂▂▃▂▂▂▃▂▂▂▂▂▂▂▁▁▁▂▁▁</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>9.93565</td></tr><tr><td>createdAt</td><td>1644227394.32657</td></tr><tr><td>loss</td><td>6.04667</td></tr><tr><td>test min loss</td><td>6.04667</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">fine-sweep-8</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/m5c3n5pz\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/m5c3n5pz</a><br/>\nFind logs at: <code>./wandb/run-20220207_094948-m5c3n5pz/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jbl686de with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/jbl686de\" target=\"_blank\">dazzling-sweep-9</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 42.569568, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 42.56956762197066\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss: 39.834874, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 39.83487421152543\n####################################################################################\nFinal Best Metric: 39.83487421152543\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 937... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▄▄▄▅▅▆▃▆▆▅▁▅▃▄▇▂▃▄▂▇▇▆▇▄▄▃█▄▁▅▆▄▂▅▅▇▅▄▇▅</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>54.99582</td></tr><tr><td>createdAt</td><td>1644227414.13867</td></tr><tr><td>loss</td><td>39.83487</td></tr><tr><td>test min loss</td><td>39.83487</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">dazzling-sweep-9</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/jbl686de\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/jbl686de</a><br/>\nFind logs at: <code>./wandb/run-20220207_095008-jbl686de/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 97ujjzi8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/97ujjzi8\" target=\"_blank\">honest-sweep-10</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    16 Batches, Step     16, Testing Loss: 44.732250, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 44.73224998007015\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    16 Batches, Step     32, Testing Loss: 45.920484, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 44.73224998007015\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    16 Batches, Step     48, Testing Loss: 45.681812, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 44.73224998007015\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    16 Batches, Step     64, Testing Loss: 46.019492, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 44.73224998007015\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    16 Batches, Step     80, Testing Loss: 45.081205, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 44.73224998007015\n####################################################################################\nFinal Best Metric: 44.73224998007015\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 975... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▄▅▅▅▆▇▂▆█▄▆▇█▁▆▄▅▄▃▅▅▅▅▄▄▄▃▃▄▅▂▇▅▆▅▆▂▅▅▅</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁</td></tr><tr><td>loss</td><td>▁▇▆█▃</td></tr><tr><td>test min loss</td><td>▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>54.03024</td></tr><tr><td>createdAt</td><td>1644227427.57905</td></tr><tr><td>loss</td><td>45.08121</td></tr><tr><td>test min loss</td><td>44.73225</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">honest-sweep-10</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/97ujjzi8\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/97ujjzi8</a><br/>\nFind logs at: <code>./wandb/run-20220207_095021-97ujjzi8/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ds04n7r2 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/ds04n7r2\" target=\"_blank\">glamorous-sweep-11</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    16 Batches, Step     16, Testing Loss: 34.812643, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 34.812642506190706\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    16 Batches, Step     32, Testing Loss: 25.072262, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 25.072262199557557\n####################################################################################\nFinal Best Metric: 25.072262199557557\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1013... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇██▇▆▆▇▆▆▆▆▆▆▄▄▅▅▃▃▄▃▃▃▂▄▂▂▂▂▂▂▁</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>32.05323</td></tr><tr><td>createdAt</td><td>1644227436.41833</td></tr><tr><td>loss</td><td>25.07226</td></tr><tr><td>test min loss</td><td>25.07226</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">glamorous-sweep-11</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/ds04n7r2\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/ds04n7r2</a><br/>\nFind logs at: <code>./wandb/run-20220207_095031-ds04n7r2/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zf63377o with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/zf63377o\" target=\"_blank\">bumbling-sweep-12</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 43.761102, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 43.76110248176419\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 44.373195, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 43.76110248176419\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    31 Batches, Step     93, Testing Loss: 42.984343, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 42.98434253614776\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    31 Batches, Step    124, Testing Loss: 43.401352, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 42.98434253614776\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    31 Batches, Step    155, Testing Loss: 43.767455, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 42.98434253614776\n####################################################################################\nFinal Best Metric: 42.98434253614776\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1051... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▄▇▄▃▅█▂▄▇▄▄▅▇▅▅▇▂▇▂▃▃▅▃▄▅█▁▅▄▃▄▅▇█▅▇█▄▃▅</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁</td></tr><tr><td>loss</td><td>▅█▁▃▅</td></tr><tr><td>test min loss</td><td>██▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>50.27427</td></tr><tr><td>createdAt</td><td>1644227516.21098</td></tr><tr><td>loss</td><td>43.76745</td></tr><tr><td>test min loss</td><td>42.98434</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">bumbling-sweep-12</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/zf63377o\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/zf63377o</a><br/>\nFind logs at: <code>./wandb/run-20220207_095148-zf63377o/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7k20nlmm with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/7k20nlmm\" target=\"_blank\">fresh-sweep-13</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 35.630892, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 35.630891527448384\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss: 32.398491, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 32.398490633283345\n####################################################################################\nFinal Best Metric: 32.398490633283345\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1089... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆█▆▆▄▅▆▄▃▆▆▄▄▅▇▄▆▆▂▄▄▂▅▃▅▄▄▄▅▃▅▃▄▅▃▃▃▃▃▁</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>27.12626</td></tr><tr><td>createdAt</td><td>1644227528.83916</td></tr><tr><td>loss</td><td>32.39849</td></tr><tr><td>test min loss</td><td>32.39849</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">fresh-sweep-13</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/7k20nlmm\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/7k20nlmm</a><br/>\nFind logs at: <code>./wandb/run-20220207_095203-7k20nlmm/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a79smkdc with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/a79smkdc\" target=\"_blank\">firm-sweep-14</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 14.680534, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 14.680533837298958\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss:  8.296329, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 8.296328817095075\n####################################################################################\nFinal Best Metric: 8.296328817095075\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1127... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆█▇▆▇▆▇▅▅▅▅▅▄▄▃▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▁▂▂▁▂▁▁▂▁▁</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>8.93157</td></tr><tr><td>createdAt</td><td>1644227538.38631</td></tr><tr><td>loss</td><td>8.29633</td></tr><tr><td>test min loss</td><td>8.29633</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">firm-sweep-14</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/a79smkdc\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/a79smkdc</a><br/>\nFind logs at: <code>./wandb/run-20220207_095213-a79smkdc/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4krs6d6k with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/4krs6d6k\" target=\"_blank\">clear-sweep-15</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 15.513124, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 15.513123629044513\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss:  6.446901, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 6.446901019738645\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    54 Batches, Step    300, Testing Loss:  5.045054, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-002   123 Batches, Step    369, Testing Loss:  3.030910, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 3.030910165942445\n####################################################################################\nemb-Evaluate Result:\nEpoch-003   123 Batches, Step    492, Testing Loss:  1.350472, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 1.350472168046601\n####################################################################################\nemb-Epoch-004     8 Batches, Step    500, Training Loss:  3.169019 (AllAvg  3.169019)\nemb-Evaluate Result:\nEpoch-004   108 Batches, Step    600, Testing Loss:  1.031486, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004   123 Batches, Step    615, Testing Loss:  0.921890, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.9218900167212194\n####################################################################################\nemb-Evaluate Result:\nEpoch-005   123 Batches, Step    738, Testing Loss:  0.660465, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.660465094507957\n####################################################################################\nemb-Evaluate Result:\nEpoch-006   123 Batches, Step    861, Testing Loss:  0.522978, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.5229779238603554\n####################################################################################\nemb-Evaluate Result:\nEpoch-007    39 Batches, Step    900, Testing Loss:  0.417519, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-007   123 Batches, Step    984, Testing Loss:  0.401438, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.4014381894043514\n####################################################################################\nemb-Epoch-008    16 Batches, Step   1000, Training Loss:  0.948686 (AllAvg  0.948686)\nemb-Evaluate Result:\nEpoch-008   123 Batches, Step   1107, Testing Loss:  0.301913, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.30191307104363735\n####################################################################################\nemb-Evaluate Result:\nEpoch-009    93 Batches, Step   1200, Testing Loss:  0.264799, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-009   123 Batches, Step   1230, Testing Loss:  0.209656, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.20965637144993762\n####################################################################################\nFinal Best Metric: 0.20965637144993762\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1165... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▅▆▅▅▄▅▄▃▃▂▂▂▁▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test min loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.28815</td></tr><tr><td>createdAt</td><td>1644227556.171</td></tr><tr><td>loss</td><td>0.20966</td></tr><tr><td>test min loss</td><td>0.20966</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">clear-sweep-15</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/4krs6d6k\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/4krs6d6k</a><br/>\nFind logs at: <code>./wandb/run-20220207_095223-4krs6d6k/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5vct3lon with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0003\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/5vct3lon\" target=\"_blank\">swift-sweep-16</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 38.116324, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 38.11632366569675\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 34.024546, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 34.02454574740663\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    31 Batches, Step     93, Testing Loss: 31.346504, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 31.34650401679837\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    31 Batches, Step    124, Testing Loss: 27.799302, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 27.799301614566726\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    31 Batches, Step    155, Testing Loss: 25.586934, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 25.586933564166632\n####################################################################################\nFinal Best Metric: 25.586933564166632\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1203... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇█▇▆▅▆▆▆▇▅▅▆▇█▄▅▅▄▅▅▅▃▃▃▃▂▂▁▂▂▃▄▂▁▁▁▁▂▂▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▆▄▂▁</td></tr><tr><td>test min loss</td><td>█▆▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>33.40409</td></tr><tr><td>createdAt</td><td>1644227570.31023</td></tr><tr><td>loss</td><td>25.58693</td></tr><tr><td>test min loss</td><td>25.58693</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">swift-sweep-16</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/5vct3lon\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/5vct3lon</a><br/>\nFind logs at: <code>./wandb/run-20220207_095244-5vct3lon/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rxh61j8s with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/rxh61j8s\" target=\"_blank\">legendary-sweep-17</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 38.634730, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 38.63473028066207\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 38.556338, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 38.556337940449616\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    31 Batches, Step     93, Testing Loss: 36.449900, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 36.4498997123874\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    31 Batches, Step    124, Testing Loss: 35.583804, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 35.583804072165975\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    31 Batches, Step    155, Testing Loss: 33.728311, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 33.7283106434102\n####################################################################################\nemb-Evaluate Result:\nEpoch-005    31 Batches, Step    186, Testing Loss: 32.937848, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 32.93784783810985\n####################################################################################\nemb-Evaluate Result:\nEpoch-006    31 Batches, Step    217, Testing Loss: 31.824076, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 31.824075815628987\n####################################################################################\nemb-Evaluate Result:\nEpoch-007    31 Batches, Step    248, Testing Loss: 30.015165, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 30.01516529005401\n####################################################################################\nemb-Evaluate Result:\nEpoch-008    31 Batches, Step    279, Testing Loss: 28.543680, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 28.54368042459293\n####################################################################################\nemb-Evaluate Result:\nEpoch-009    21 Batches, Step    300, Testing Loss: 28.167372, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-009    31 Batches, Step    310, Testing Loss: 28.676479, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 28.16737174987793\n####################################################################################\nFinal Best Metric: 28.16737174987793\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1241... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇█▅▅▄▅▅▆▄▅▆▄▅▄▅▄▄▄▄▅▄▄▄▅▄▂▂▄▂▄▂▃▅▁▃▂▃▂▃▃</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>██▇▆▅▄▃▂▁▁▁</td></tr><tr><td>test min loss</td><td>██▇▆▅▄▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>39.29911</td></tr><tr><td>createdAt</td><td>1644227583.74723</td></tr><tr><td>loss</td><td>28.67648</td></tr><tr><td>test min loss</td><td>28.16737</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">legendary-sweep-17</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/rxh61j8s\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/rxh61j8s</a><br/>\nFind logs at: <code>./wandb/run-20220207_095254-rxh61j8s/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eghilw1d with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/eghilw1d\" target=\"_blank\">soft-sweep-18</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 21.989595, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 21.989595023953186\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss: 13.528207, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 13.528206630628937\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    62 Batches, Step    186, Testing Loss:  8.918148, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 8.91814830351849\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    62 Batches, Step    248, Testing Loss:  6.199609, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 6.1996086178993695\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    52 Batches, Step    300, Testing Loss:  4.249839, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004    62 Batches, Step    310, Testing Loss:  3.935044, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 3.935043670693222\n####################################################################################\nemb-Evaluate Result:\nEpoch-005    62 Batches, Step    372, Testing Loss:  2.465427, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 2.4654270094268176\n####################################################################################\nemb-Evaluate Result:\nEpoch-006    62 Batches, Step    434, Testing Loss:  1.899021, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 1.8990206158891016\n####################################################################################\nemb-Evaluate Result:\nEpoch-007    62 Batches, Step    496, Testing Loss:  1.367292, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 1.3672924674287135\n####################################################################################\nemb-Epoch-008     4 Batches, Step    500, Training Loss:  2.512884 (AllAvg  2.512884)\nemb-Evaluate Result:\nEpoch-008    62 Batches, Step    558, Testing Loss:  0.944802, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.9448017392839704\n####################################################################################\nemb-Evaluate Result:\nEpoch-009    42 Batches, Step    600, Testing Loss:  0.781844, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-009    62 Batches, Step    620, Testing Loss:  0.835622, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.7818438057996788\n####################################################################################\nFinal Best Metric: 0.7818438057996788\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1279... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▁▁▁▁▁</td></tr><tr><td>test min loss</td><td>█▅▄▃▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>2.95147</td></tr><tr><td>createdAt</td><td>1644227599.56514</td></tr><tr><td>loss</td><td>0.83562</td></tr><tr><td>test min loss</td><td>0.78184</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">soft-sweep-18</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/eghilw1d\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/eghilw1d</a><br/>\nFind logs at: <code>./wandb/run-20220207_095309-eghilw1d/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 72w42twa with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/72w42twa\" target=\"_blank\">clear-sweep-19</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 36.441252, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 36.44125187153719\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss: 33.934738, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 33.934738081328724\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    62 Batches, Step    186, Testing Loss: 28.902053, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 28.90205258739238\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    62 Batches, Step    248, Testing Loss: 27.693088, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 27.693087947611907\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    52 Batches, Step    300, Testing Loss: 23.537716, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004    62 Batches, Step    310, Testing Loss: 23.717674, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 23.537715756163305\n####################################################################################\nFinal Best Metric: 23.537715756163305\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1317... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇▇▆▅█▄▆▄▇▅▆▆▅▂▆▃▅▄▅▅▅▄▃▄▃▃▂▁▂▃▃▄▄▂▁▂▂▃▃▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▇▄▃▁▁</td></tr><tr><td>test min loss</td><td>█▇▄▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>30.76309</td></tr><tr><td>createdAt</td><td>1644227611.75597</td></tr><tr><td>loss</td><td>23.71767</td></tr><tr><td>test min loss</td><td>23.53772</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">clear-sweep-19</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/72w42twa\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/72w42twa</a><br/>\nFind logs at: <code>./wandb/run-20220207_095325-72w42twa/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xkcy37bs with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/xkcy37bs\" target=\"_blank\">polar-sweep-20</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss:  0.686825, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.6868249372560151\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss:  0.175636, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.17563582926380392\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    62 Batches, Step    186, Testing Loss:  0.125569, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.12556873368365423\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    62 Batches, Step    248, Testing Loss:  0.080217, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.08021739733462431\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    52 Batches, Step    300, Testing Loss:  0.067215, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004    62 Batches, Step    310, Testing Loss:  0.065777, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.06577679492077049\n####################################################################################\nemb-Evaluate Result:\nEpoch-005    62 Batches, Step    372, Testing Loss:  0.051877, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.05187670468371742\n####################################################################################\nemb-Evaluate Result:\nEpoch-006    62 Batches, Step    434, Testing Loss:  0.057810, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.05187670468371742\n####################################################################################\nemb-Evaluate Result:\nEpoch-007    62 Batches, Step    496, Testing Loss:  0.063282, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.05187670468371742\n####################################################################################\nemb-Epoch-008     4 Batches, Step    500, Training Loss:  0.132034 (AllAvg  0.132034)\nemb-Evaluate Result:\nEpoch-008    62 Batches, Step    558, Testing Loss:  0.029493, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.02949318019863294\n####################################################################################\nemb-Evaluate Result:\nEpoch-009    42 Batches, Step    600, Testing Loss:  0.026451, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-009    62 Batches, Step    620, Testing Loss:  0.037518, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.026451024264857476\n####################################################################################\nFinal Best Metric: 0.026451024264857476\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1355... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>test min loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.11284</td></tr><tr><td>createdAt</td><td>1644227630.38059</td></tr><tr><td>loss</td><td>0.03752</td></tr><tr><td>test min loss</td><td>0.02645</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">polar-sweep-20</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/xkcy37bs\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/xkcy37bs</a><br/>\nFind logs at: <code>./wandb/run-20220207_095340-xkcy37bs/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kdouju5p with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/kdouju5p\" target=\"_blank\">frosty-sweep-21</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 38.889527, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 38.88952737925004\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss: 38.005743, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 38.00574329921177\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    62 Batches, Step    186, Testing Loss: 36.229213, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 36.229212858239\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    62 Batches, Step    248, Testing Loss: 37.663685, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 36.229212858239\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    52 Batches, Step    300, Testing Loss: 37.022542, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004    62 Batches, Step    310, Testing Loss: 36.825093, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 36.229212858239\n####################################################################################\nFinal Best Metric: 36.229212858239\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1393... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇▄▄▆▅▅▅▆▅▇▅▅▅▃▆▆▅▅▅▆▃▃▅█▃▁▄▆▃▃▄▄▂▄▁▃▃▂▄▅</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▆▁▅▃▃</td></tr><tr><td>test min loss</td><td>█▆▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>51.20841</td></tr><tr><td>createdAt</td><td>1644227643.41944</td></tr><tr><td>loss</td><td>36.82509</td></tr><tr><td>test min loss</td><td>36.22921</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">frosty-sweep-21</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/kdouju5p\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/kdouju5p</a><br/>\nFind logs at: <code>./wandb/run-20220207_095356-kdouju5p/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ckax2ri6 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/ckax2ri6\" target=\"_blank\">visionary-sweep-22</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 38.699595, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 38.699595003711934\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 37.524359, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 37.52435855476224\n####################################################################################\nFinal Best Metric: 37.52435855476224\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1431... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▅▄▃█▆▆▆█▆▆▆▅▅▅▅▅▇▅▅▆▅▅▅▄▅▆▄▇▅▅▄▁▆▆▄▅▆▅▆▇</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>54.40343</td></tr><tr><td>createdAt</td><td>1644227656.1434</td></tr><tr><td>loss</td><td>37.52436</td></tr><tr><td>test min loss</td><td>37.52436</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">visionary-sweep-22</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/ckax2ri6\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/ckax2ri6</a><br/>\nFind logs at: <code>./wandb/run-20220207_095411-ckax2ri6/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: czi65n8o with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/czi65n8o\" target=\"_blank\">good-sweep-23</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 42.462829, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 42.462828811334106\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss: 39.459662, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 39.45966230119978\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    62 Batches, Step    186, Testing Loss: 41.156442, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 39.45966230119978\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    62 Batches, Step    248, Testing Loss: 39.239374, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 39.239374433244976\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    52 Batches, Step    300, Testing Loss: 38.755366, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004    62 Batches, Step    310, Testing Loss: 38.076549, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 38.076548946147064\n####################################################################################\nFinal Best Metric: 38.076548946147064\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1469... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆█▂█▅▅▃█▆▆▆▅▄▁▄▅▅▅▃▆▁▅▃▇▃▃▅▃▅▂▅▄▂▃▄▄▄▇▃▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▃▆▃▂▁</td></tr><tr><td>test min loss</td><td>█▃▃▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>36.74872</td></tr><tr><td>createdAt</td><td>1644227668.56502</td></tr><tr><td>loss</td><td>38.07655</td></tr><tr><td>test min loss</td><td>38.07655</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">good-sweep-23</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/czi65n8o\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/czi65n8o</a><br/>\nFind logs at: <code>./wandb/run-20220207_095421-czi65n8o/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f7xepc1c with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/f7xepc1c\" target=\"_blank\">fluent-sweep-24</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 38.172929, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 38.17292886850785\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 36.577497, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 36.57749701519402\n####################################################################################\nFinal Best Metric: 36.57749701519402\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1507... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▅▄▅▄▅▄▆▇▄▂▆▅▄▅▄▄▂▄▁▄▆▅▆▁█▅▄▃▁▄▃▅▄▄▄▂▄▂▄▅</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>48.41273</td></tr><tr><td>createdAt</td><td>1644227681.95067</td></tr><tr><td>loss</td><td>36.5775</td></tr><tr><td>test min loss</td><td>36.5775</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">fluent-sweep-24</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/f7xepc1c\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/f7xepc1c</a><br/>\nFind logs at: <code>./wandb/run-20220207_095437-f7xepc1c/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fm1fhexd with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/fm1fhexd\" target=\"_blank\">pretty-sweep-25</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 40.026932, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 40.0269322298011\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss: 37.994601, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 37.9946011913066\n####################################################################################\nFinal Best Metric: 37.9946011913066\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1545... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▁▄▃▇█▃▃▂▄▃▆▃▃▃▁▆▆▂▅▅▁▄▆▅▅▄▃▇▃▂▆▃▃▂▄▂▂▄▄▂</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>42.15575</td></tr><tr><td>createdAt</td><td>1644227692.71858</td></tr><tr><td>loss</td><td>37.9946</td></tr><tr><td>test min loss</td><td>37.9946</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">pretty-sweep-25</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/fm1fhexd\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/fm1fhexd</a><br/>\nFind logs at: <code>./wandb/run-20220207_095447-fm1fhexd/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f5qty2zc with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/f5qty2zc\" target=\"_blank\">effortless-sweep-26</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss:  4.523802, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 4.5238019495594255\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss:  0.520946, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.5209455520522838\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    31 Batches, Step     93, Testing Loss:  0.264504, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.2645042590340789\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    31 Batches, Step    124, Testing Loss:  0.132524, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.1325236089071449\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    31 Batches, Step    155, Testing Loss:  0.136638, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.1325236089071449\n####################################################################################\nFinal Best Metric: 0.1325236089071449\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1583... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▂▁▁▁</td></tr><tr><td>test min loss</td><td>█▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.31016</td></tr><tr><td>createdAt</td><td>1644227704.94928</td></tr><tr><td>loss</td><td>0.13664</td></tr><tr><td>test min loss</td><td>0.13252</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">effortless-sweep-26</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/f5qty2zc\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/f5qty2zc</a><br/>\nFind logs at: <code>./wandb/run-20220207_095457-f5qty2zc/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j16v7bpx with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0003\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/j16v7bpx\" target=\"_blank\">proud-sweep-27</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 24.421756, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 24.421755615545777\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss: 15.031063, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 15.031063469088807\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    54 Batches, Step    300, Testing Loss: 12.895652, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-002   123 Batches, Step    369, Testing Loss: 10.344270, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 10.34426953841229\n####################################################################################\nemb-Evaluate Result:\nEpoch-003   123 Batches, Step    492, Testing Loss:  7.887949, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 7.887948736852529\n####################################################################################\nemb-Epoch-004     8 Batches, Step    500, Training Loss:  8.565044 (AllAvg  8.565044)\nemb-Evaluate Result:\nEpoch-004   108 Batches, Step    600, Testing Loss:  6.214306, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004   123 Batches, Step    615, Testing Loss:  6.017140, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 6.017139814337906\n####################################################################################\nFinal Best Metric: 6.017139814337906\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1621... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇█▇▅▆▅▅▄▇▅▃▃▃▂▃▂▂▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▄▃▂▁▁</td></tr><tr><td>test min loss</td><td>█▄▄▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>9.41064</td></tr><tr><td>createdAt</td><td>1644227720.6763</td></tr><tr><td>loss</td><td>6.01714</td></tr><tr><td>test min loss</td><td>6.01714</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">proud-sweep-27</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/j16v7bpx\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/j16v7bpx</a><br/>\nFind logs at: <code>./wandb/run-20220207_095512-j16v7bpx/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2wldmuol with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/2wldmuol\" target=\"_blank\">stellar-sweep-28</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 38.390244, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 38.39024423093212\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 39.100662, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 38.39024423093212\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    31 Batches, Step     93, Testing Loss: 37.607934, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 37.60793351153938\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    31 Batches, Step    124, Testing Loss: 36.542887, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 36.542887162189096\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    31 Batches, Step    155, Testing Loss: 36.524254, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 36.52425376736388\n####################################################################################\nFinal Best Metric: 36.52425376736388\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1659... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇▆▆▇▅▇▇▆▆▄▄▂█▄▆▄▄▆▄▃▄▆▅▅▄▄▅▁▅▁▄▁▄▅▅▂▃▁▇▃</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁</td></tr><tr><td>loss</td><td>▆█▄▁▁</td></tr><tr><td>test min loss</td><td>██▅▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>49.06225</td></tr><tr><td>createdAt</td><td>1644227734.59989</td></tr><tr><td>loss</td><td>36.52425</td></tr><tr><td>test min loss</td><td>36.52425</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">stellar-sweep-28</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/2wldmuol\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/2wldmuol</a><br/>\nFind logs at: <code>./wandb/run-20220207_095528-2wldmuol/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bfya65ny with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/bfya65ny\" target=\"_blank\">happy-sweep-29</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 40.989153, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 40.98915325865454\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss: 35.000041, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 35.00004056035256\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    54 Batches, Step    300, Testing Loss: 34.298331, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-002   123 Batches, Step    369, Testing Loss: 31.854867, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 31.854866572788783\n####################################################################################\nemb-Evaluate Result:\nEpoch-003   123 Batches, Step    492, Testing Loss: 26.592951, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 26.592951326954122\n####################################################################################\nemb-Epoch-004     8 Batches, Step    500, Training Loss: 34.055922 (AllAvg 34.055922)\nemb-Evaluate Result:\nEpoch-004   108 Batches, Step    600, Testing Loss: 24.772507, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004   123 Batches, Step    615, Testing Loss: 23.689780, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 23.68978029367875\n####################################################################################\nFinal Best Metric: 23.68978029367875\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1697... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▅▇▇▆█▆█▅▄▂▅▄▆▅▄▆▂▃▂▃▃▂▃▅▆▄▄▂▂▂▁▄▃▁▂▁▃▃▁▃</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▆▅▄▂▁▁</td></tr><tr><td>test min loss</td><td>█▆▅▄▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>41.76336</td></tr><tr><td>createdAt</td><td>1644227751.89309</td></tr><tr><td>loss</td><td>23.68978</td></tr><tr><td>test min loss</td><td>23.68978</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">happy-sweep-29</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/bfya65ny\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/bfya65ny</a><br/>\nFind logs at: <code>./wandb/run-20220207_095543-bfya65ny/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: afvyawc8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/afvyawc8\" target=\"_blank\">deft-sweep-30</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    16 Batches, Step     16, Testing Loss: 38.800230, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 38.80022998731963\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    16 Batches, Step     32, Testing Loss: 37.095081, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 37.09508148504763\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    16 Batches, Step     48, Testing Loss: 35.355648, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 35.35564835217534\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    16 Batches, Step     64, Testing Loss: 34.147328, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 34.14732847408373\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    16 Batches, Step     80, Testing Loss: 33.128344, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 33.12834389355718\n####################################################################################\nFinal Best Metric: 33.12834389355718\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1735... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆▄▅▅▄▅▄▅▃█▅▇▄▄▄▃▄▄▆▅▅▄▃▃▄▂▃▃▃▁▂▃▃▃▃▃▃▃▂▂</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▆▄▂▁</td></tr><tr><td>test min loss</td><td>█▆▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>41.07902</td></tr><tr><td>createdAt</td><td>1644227765.52049</td></tr><tr><td>loss</td><td>33.12834</td></tr><tr><td>test min loss</td><td>33.12834</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">deft-sweep-30</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/afvyawc8\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/afvyawc8</a><br/>\nFind logs at: <code>./wandb/run-20220207_095558-afvyawc8/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ak7utj7c with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0003\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/ak7utj7c\" target=\"_blank\">frosty-sweep-31</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 23.286621, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 23.28662109375\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss: 14.596101, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 14.596100885040906\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    54 Batches, Step    300, Testing Loss: 12.417182, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-002   123 Batches, Step    369, Testing Loss: 10.578456, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 10.578455613583934\n####################################################################################\nemb-Evaluate Result:\nEpoch-003   123 Batches, Step    492, Testing Loss:  8.057373, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 8.057373192845558\n####################################################################################\nemb-Epoch-004     8 Batches, Step    500, Training Loss:  9.520250 (AllAvg  9.520250)\nemb-Evaluate Result:\nEpoch-004   108 Batches, Step    600, Testing Loss:  6.228994, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004   123 Batches, Step    615, Testing Loss:  5.868367, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 5.868366620978531\n####################################################################################\nemb-Evaluate Result:\nEpoch-005   123 Batches, Step    738, Testing Loss:  5.024686, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 5.024686297591852\n####################################################################################\nemb-Evaluate Result:\nEpoch-006   123 Batches, Step    861, Testing Loss:  4.161285, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 4.161285380927884\n####################################################################################\nemb-Evaluate Result:\nEpoch-007    39 Batches, Step    900, Testing Loss:  4.018262, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-007   123 Batches, Step    984, Testing Loss:  3.413903, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 3.4139031098813426\n####################################################################################\nemb-Epoch-008    16 Batches, Step   1000, Training Loss:  3.791905 (AllAvg  3.791905)\nemb-Evaluate Result:\nEpoch-008   123 Batches, Step   1107, Testing Loss:  2.803948, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 2.8039475752382863\n####################################################################################\nemb-Evaluate Result:\nEpoch-009    93 Batches, Step   1200, Testing Loss:  2.548156, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-009   123 Batches, Step   1230, Testing Loss:  2.455742, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 2.4557423688927473\n####################################################################################\nFinal Best Metric: 2.4557423688927473\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1773... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇█▅▅▅▄▄▄▃▃▃▃▂▂▃▂▂▂▁▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>test min loss</td><td>█▅▄▄▃▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>3.32982</td></tr><tr><td>createdAt</td><td>1644227786.2351</td></tr><tr><td>loss</td><td>2.45574</td></tr><tr><td>test min loss</td><td>2.45574</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">frosty-sweep-31</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/ak7utj7c\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/ak7utj7c</a><br/>\nFind logs at: <code>./wandb/run-20220207_095614-ak7utj7c/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tolve4et with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/tolve4et\" target=\"_blank\">vocal-sweep-32</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    16 Batches, Step     16, Testing Loss: 43.472393, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 43.47239326944157\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    16 Batches, Step     32, Testing Loss: 42.370609, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 42.37060866063955\n####################################################################################\nFinal Best Metric: 42.37060866063955\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1811... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▃▃▄▄▃▄▃▄▅▃▂▁▄▄▆▇█▆▂▅▆▄▆▄▂▃▅▃▃▄▄▂</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>48.14716</td></tr><tr><td>createdAt</td><td>1644227799.69885</td></tr><tr><td>loss</td><td>42.37061</td></tr><tr><td>test min loss</td><td>42.37061</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">vocal-sweep-32</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/tolve4et\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/tolve4et</a><br/>\nFind logs at: <code>./wandb/run-20220207_095635-tolve4et/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f9t66713 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/f9t66713\" target=\"_blank\">sandy-sweep-33</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    16 Batches, Step     16, Testing Loss: 10.146311, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 10.14631140961939\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    16 Batches, Step     32, Testing Loss:  3.626403, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 3.6264027527400424\n####################################################################################\nFinal Best Metric: 3.6264027527400424\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1849... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>5.61983</td></tr><tr><td>createdAt</td><td>1644227810.11046</td></tr><tr><td>loss</td><td>3.6264</td></tr><tr><td>test min loss</td><td>3.6264</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">sandy-sweep-33</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/f9t66713\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/f9t66713</a><br/>\nFind logs at: <code>./wandb/run-20220207_095645-f9t66713/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: givybk8j with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/givybk8j\" target=\"_blank\">dashing-sweep-34</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 25.316625, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 25.31662481658313\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 15.978757, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 15.978757254931391\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    31 Batches, Step     93, Testing Loss: 11.897846, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 11.897845832669006\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    31 Batches, Step    124, Testing Loss:  9.249251, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 9.249251054257762\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    31 Batches, Step    155, Testing Loss:  7.437358, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 7.437358029034673\n####################################################################################\nFinal Best Metric: 7.437358029034673\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1887... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>██▇▆▆▆▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▃▂▁</td></tr><tr><td>test min loss</td><td>█▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>7.34634</td></tr><tr><td>createdAt</td><td>1644227821.77727</td></tr><tr><td>loss</td><td>7.43736</td></tr><tr><td>test min loss</td><td>7.43736</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">dashing-sweep-34</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/givybk8j\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/givybk8j</a><br/>\nFind logs at: <code>./wandb/run-20220207_095655-givybk8j/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: utkpqogn with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0003\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/utkpqogn\" target=\"_blank\">kind-sweep-35</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 33.644253, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 33.644252816025094\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss: 23.372448, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 23.372447772901886\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    54 Batches, Step    300, Testing Loss: 18.619331, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-002   123 Batches, Step    369, Testing Loss: 17.436648, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 17.43664821313352\n####################################################################################\nemb-Evaluate Result:\nEpoch-003   123 Batches, Step    492, Testing Loss: 12.947396, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 12.947396142142159\n####################################################################################\nemb-Epoch-004     8 Batches, Step    500, Training Loss: 17.381576 (AllAvg 17.381576)\nemb-Evaluate Result:\nEpoch-004   108 Batches, Step    600, Testing Loss: 10.243494, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004   123 Batches, Step    615, Testing Loss:  9.788772, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 9.788771921274613\n####################################################################################\nemb-Evaluate Result:\nEpoch-005   123 Batches, Step    738, Testing Loss:  7.586171, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 7.586171111282037\n####################################################################################\nemb-Evaluate Result:\nEpoch-006   123 Batches, Step    861, Testing Loss:  6.910416, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 6.910415873235586\n####################################################################################\nemb-Evaluate Result:\nEpoch-007    39 Batches, Step    900, Testing Loss:  5.869064, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-007   123 Batches, Step    984, Testing Loss:  4.996153, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 4.996153179480105\n####################################################################################\nemb-Epoch-008    16 Batches, Step   1000, Training Loss:  8.274610 (AllAvg  8.274610)\nemb-Evaluate Result:\nEpoch-008   123 Batches, Step   1107, Testing Loss:  3.799654, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 3.799654396212831\n####################################################################################\nemb-Evaluate Result:\nEpoch-009    93 Batches, Step   1200, Testing Loss:  3.158502, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-009   123 Batches, Step   1230, Testing Loss:  3.191303, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 3.1585020191815434\n####################################################################################\nFinal Best Metric: 3.1585020191815434\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1925... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆█▆▆▆▃▆▅▄▄▄▃▄▄▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▆▅▄▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>test min loss</td><td>█▆▅▄▃▃▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>5.24573</td></tr><tr><td>createdAt</td><td>1644227839.43503</td></tr><tr><td>loss</td><td>3.1913</td></tr><tr><td>test min loss</td><td>3.1585</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">kind-sweep-35</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/utkpqogn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/utkpqogn</a><br/>\nFind logs at: <code>./wandb/run-20220207_095706-utkpqogn/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d9syd8x6 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/d9syd8x6\" target=\"_blank\">scarlet-sweep-36</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    16 Batches, Step     16, Testing Loss: 40.575614, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 40.57561438424246\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    16 Batches, Step     32, Testing Loss: 41.291892, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 40.57561438424246\n####################################################################################\nFinal Best Metric: 40.57561438424246\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1963... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▁▃▃▇▄▅▄▁▅▆▄▆▃▄▆▄▄▆▃▁▄▅▂▆▃▄▇▃▄▄█▁</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>▁█</td></tr><tr><td>test min loss</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>47.52453</td></tr><tr><td>createdAt</td><td>1644227851.32526</td></tr><tr><td>loss</td><td>41.29189</td></tr><tr><td>test min loss</td><td>40.57561</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">scarlet-sweep-36</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/d9syd8x6\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/d9syd8x6</a><br/>\nFind logs at: <code>./wandb/run-20220207_095726-d9syd8x6/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dmdtde9d with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/dmdtde9d\" target=\"_blank\">solar-sweep-37</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss:  3.738032, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 3.73803220476423\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss:  0.594993, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.5949928182728437\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    31 Batches, Step     93, Testing Loss:  0.540260, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.5402596909172681\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    31 Batches, Step    124, Testing Loss:  0.523409, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.5234086610832993\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    31 Batches, Step    155, Testing Loss:  0.520462, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.5204618883376219\n####################################################################################\nFinal Best Metric: 0.5204618883376219\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2001... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>██▆▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▁▁▁▁</td></tr><tr><td>test min loss</td><td>█▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.73986</td></tr><tr><td>createdAt</td><td>1644227862.90798</td></tr><tr><td>loss</td><td>0.52046</td></tr><tr><td>test min loss</td><td>0.52046</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">solar-sweep-37</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/dmdtde9d\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/dmdtde9d</a><br/>\nFind logs at: <code>./wandb/run-20220207_095736-dmdtde9d/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f2jvjgjp with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/f2jvjgjp\" target=\"_blank\">clean-sweep-38</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 43.143185, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 43.14318544037488\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss: 42.727394, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 42.72739387045101\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    62 Batches, Step    186, Testing Loss: 41.566258, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 41.56625771035954\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    62 Batches, Step    248, Testing Loss: 40.278098, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 40.27809758089027\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    52 Batches, Step    300, Testing Loss: 39.888845, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004    62 Batches, Step    310, Testing Loss: 40.390971, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 39.8888454048001\n####################################################################################\nFinal Best Metric: 39.8888454048001\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2039... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆▃▄█▄▆▆▆▆▄▅▃▅▂▄▅▅▄▆▅▆▅▅▅▄▃▄█▄▄▅▄▁▅▂▂▄▄▂▅</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▇▅▂▁▂</td></tr><tr><td>test min loss</td><td>█▇▅▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>47.26676</td></tr><tr><td>createdAt</td><td>1644227879.28949</td></tr><tr><td>loss</td><td>40.39097</td></tr><tr><td>test min loss</td><td>39.88885</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">clean-sweep-38</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/f2jvjgjp\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/f2jvjgjp</a><br/>\nFind logs at: <code>./wandb/run-20220207_095752-f2jvjgjp/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8k1biwvz with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/8k1biwvz\" target=\"_blank\">ruby-sweep-39</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss:  0.526993, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 0.526992684116169\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss:  0.509008, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.5090080329350063\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    54 Batches, Step    300, Testing Loss:  0.510538, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-002   123 Batches, Step    369, Testing Loss:  0.499214, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.4992143384047917\n####################################################################################\nemb-Evaluate Result:\nEpoch-003   123 Batches, Step    492, Testing Loss:  0.491584, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.4915840692666112\n####################################################################################\nemb-Epoch-004     8 Batches, Step    500, Training Loss:  0.568736 (AllAvg  0.568736)\nemb-Evaluate Result:\nEpoch-004   108 Batches, Step    600, Testing Loss:  0.492880, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004   123 Batches, Step    615, Testing Loss:  0.494756, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.4915840692666112\n####################################################################################\nFinal Best Metric: 0.4915840692666112\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2077... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▅▃▁▁▂</td></tr><tr><td>test min loss</td><td>█▄▄▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.23944</td></tr><tr><td>createdAt</td><td>1644227895.98116</td></tr><tr><td>loss</td><td>0.49476</td></tr><tr><td>test min loss</td><td>0.49158</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">ruby-sweep-39</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/8k1biwvz\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/8k1biwvz</a><br/>\nFind logs at: <code>./wandb/run-20220207_095807-8k1biwvz/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wz5m8tuo with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/wz5m8tuo\" target=\"_blank\">earnest-sweep-40</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 37.563401, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 37.56340128061723\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss: 33.635115, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 33.63511505905463\n####################################################################################\nFinal Best Metric: 33.63511505905463\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2115... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▅▇▅▆▅▃▅▇▅▅▂▄▆▃▁▂▄▅▅█▅▇▆▄▂▅▂▂▃▃▂▄▆▆▃▃▃▃▄▂</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>34.31086</td></tr><tr><td>createdAt</td><td>1644227908.79834</td></tr><tr><td>loss</td><td>33.63512</td></tr><tr><td>test min loss</td><td>33.63512</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">earnest-sweep-40</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/wz5m8tuo\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/wz5m8tuo</a><br/>\nFind logs at: <code>./wandb/run-20220207_095823-wz5m8tuo/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q18fyx2o with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/q18fyx2o\" target=\"_blank\">atomic-sweep-41</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 36.137712, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 36.137711972606425\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss: 33.062596, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 33.062596145941285\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    54 Batches, Step    300, Testing Loss: 29.335758, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-002   123 Batches, Step    369, Testing Loss: 27.060223, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 27.060223170689174\n####################################################################################\nemb-Evaluate Result:\nEpoch-003   123 Batches, Step    492, Testing Loss: 25.836870, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 25.836870388108856\n####################################################################################\nemb-Epoch-004     8 Batches, Step    500, Training Loss: 36.743015 (AllAvg 36.743015)\nemb-Evaluate Result:\nEpoch-004   108 Batches, Step    600, Testing Loss: 23.017402, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-004   123 Batches, Step    615, Testing Loss: 22.685391, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 22.68539062811404\n####################################################################################\nemb-Evaluate Result:\nEpoch-005   123 Batches, Step    738, Testing Loss: 20.603147, Used Time:   0.1m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 20.60314653357681\n####################################################################################\nemb-Evaluate Result:\nEpoch-006   123 Batches, Step    861, Testing Loss: 18.256408, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 18.256407601492747\n####################################################################################\nemb-Evaluate Result:\nEpoch-007    39 Batches, Step    900, Testing Loss: 17.653971, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-007   123 Batches, Step    984, Testing Loss: 16.349810, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 16.349809568755482\n####################################################################################\nemb-Epoch-008    16 Batches, Step   1000, Training Loss: 22.315399 (AllAvg 22.315399)\nemb-Evaluate Result:\nEpoch-008   123 Batches, Step   1107, Testing Loss: 14.794219, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 14.794219425746373\n####################################################################################\nemb-Evaluate Result:\nEpoch-009    93 Batches, Step   1200, Testing Loss: 15.018108, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nemb-Evaluate Result:\nEpoch-009   123 Batches, Step   1230, Testing Loss: 13.174639, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 13.174638611929756\n####################################################################################\nFinal Best Metric: 13.174638611929756\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2153... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆█▆▅▆▄▄▆▅▅▄▇▃▄▃▄▃▁▃▄▃▃▄▂▂▂▂▁▂▂▂▁▂▂▂▁▂▂▂▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▇▆▅▅▄▄▃▃▂▂▁▂▁</td></tr><tr><td>test min loss</td><td>█▇▆▅▅▄▄▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>15.89796</td></tr><tr><td>createdAt</td><td>1644227931.14008</td></tr><tr><td>loss</td><td>13.17464</td></tr><tr><td>test min loss</td><td>13.17464</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">atomic-sweep-41</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/q18fyx2o\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/q18fyx2o</a><br/>\nFind logs at: <code>./wandb/run-20220207_095838-q18fyx2o/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jxrl96wm with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/jxrl96wm\" target=\"_blank\">clear-sweep-42</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 42.636641, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 42.63664082118443\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 43.412254, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 42.63664082118443\n####################################################################################\nFinal Best Metric: 42.63664082118443\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2191... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▄▄▆▄▃▂▂▄▇▃▄▃▃▅▆▃▁▅▃▅▄▅▃▆▆▆▅▄▅▃▄▃▆▃▄▃█▂▃▄</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>▁█</td></tr><tr><td>test min loss</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>51.3815</td></tr><tr><td>createdAt</td><td>1644227943.68976</td></tr><tr><td>loss</td><td>43.41225</td></tr><tr><td>test min loss</td><td>42.63664</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">clear-sweep-42</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/jxrl96wm\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/jxrl96wm</a><br/>\nFind logs at: <code>./wandb/run-20220207_095858-jxrl96wm/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3p9k3pnv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/3p9k3pnv\" target=\"_blank\">avid-sweep-43</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 49.212518, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 49.21251779673051\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 47.030400, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 47.03040049027423\n####################################################################################\nFinal Best Metric: 47.03040049027423\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2229... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇▃▅▅▆▃▁▄▇▆▇▇▅▆▆▅▇▄▄▇▃▇█▄▃▆▄▆▅▅▅▄▅▅▄▅▄▄▅█</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>63.36187</td></tr><tr><td>createdAt</td><td>1644227954.33686</td></tr><tr><td>loss</td><td>47.0304</td></tr><tr><td>test min loss</td><td>47.0304</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">avid-sweep-43</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/3p9k3pnv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/3p9k3pnv</a><br/>\nFind logs at: <code>./wandb/run-20220207_095908-3p9k3pnv/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vmf36pls with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/vmf36pls\" target=\"_blank\">balmy-sweep-44</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss:  0.228618, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.22861811397026996\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss:  0.151385, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.1513846258119661\n####################################################################################\nFinal Best Metric: 0.1513846258119661\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2267... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▇▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.21496</td></tr><tr><td>createdAt</td><td>1644227964.91884</td></tr><tr><td>loss</td><td>0.15138</td></tr><tr><td>test min loss</td><td>0.15138</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">balmy-sweep-44</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/vmf36pls\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/vmf36pls</a><br/>\nFind logs at: <code>./wandb/run-20220207_095919-vmf36pls/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jwq2ywd3 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/jwq2ywd3\" target=\"_blank\">misty-sweep-45</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    16 Batches, Step     16, Testing Loss: 10.119481, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 10.119481359209333\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    16 Batches, Step     32, Testing Loss:  3.329050, Used Time:   0.0m, Remaining Time:   0.1m\n-------------------------------------------------------------------------------\nBest Metric: 3.3290496261752383\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    16 Batches, Step     48, Testing Loss:  0.774926, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.7749259557042804\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    16 Batches, Step     64, Testing Loss:  0.391070, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.39107005024442865\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    16 Batches, Step     80, Testing Loss:  0.234422, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.234422256447831\n####################################################################################\nemb-Evaluate Result:\nEpoch-005    16 Batches, Step     96, Testing Loss:  0.210374, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.210374221813922\n####################################################################################\nemb-Evaluate Result:\nEpoch-006    16 Batches, Step    112, Testing Loss:  0.132042, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.13204199820756912\n####################################################################################\nemb-Evaluate Result:\nEpoch-007    16 Batches, Step    128, Testing Loss:  0.134119, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.13204199820756912\n####################################################################################\nemb-Evaluate Result:\nEpoch-008    16 Batches, Step    144, Testing Loss:  0.109528, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.1095275384735088\n####################################################################################\nemb-Evaluate Result:\nEpoch-009    16 Batches, Step    160, Testing Loss:  0.099579, Used Time:   0.1m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.0995790100827509\n####################################################################################\nFinal Best Metric: 0.0995790100827509\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2305... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▆▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▃▁▁▁▁▁▁▁▁</td></tr><tr><td>test min loss</td><td>█▃▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.25712</td></tr><tr><td>createdAt</td><td>1644227977.53478</td></tr><tr><td>loss</td><td>0.09958</td></tr><tr><td>test min loss</td><td>0.09958</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">misty-sweep-45</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/jwq2ywd3\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/jwq2ywd3</a><br/>\nFind logs at: <code>./wandb/run-20220207_095929-jwq2ywd3/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 30kknxz8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_epoch: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_lr: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/30kknxz8\" target=\"_blank\">flowing-sweep-46</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/r48kqhtn</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss:  3.582764, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 3.5827644406532753\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss:  0.644058, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.6440582317965371\n####################################################################################\nemb-Evaluate Result:\nEpoch-002    31 Batches, Step     93, Testing Loss:  0.544227, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.5442271481971351\n####################################################################################\nemb-Evaluate Result:\nEpoch-003    31 Batches, Step    124, Testing Loss:  0.528952, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.5289523248769799\n####################################################################################\nemb-Evaluate Result:\nEpoch-004    31 Batches, Step    155, Testing Loss:  0.521812, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 0.5218123227966075\n####################################################################################\nFinal Best Metric: 0.5218123227966075\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2343... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▆▆▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>createdAt</td><td>▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▁▁▁▁</td></tr><tr><td>test min loss</td><td>█▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.53422</td></tr><tr><td>createdAt</td><td>1644227990.94782</td></tr><tr><td>loss</td><td>0.52181</td></tr><tr><td>test min loss</td><td>0.52181</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">flowing-sweep-46</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/30kknxz8\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/30kknxz8</a><br/>\nFind logs at: <code>./wandb/run-20220207_095944-30kknxz8/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=de072003-a9db-4342-8067-19a4b45feff1' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "942b6ea3-265d-437d-881e-5f3718d11bf2",
  "deepnote_execution_queue": []
 }
}