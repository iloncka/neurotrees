{
 "cells": [
  {
   "cell_type": "code",
   "source": "import wandb",
   "metadata": {
    "cell_id": "f146784c-0402-4ef4-b2fd-40bfe27860ec",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "47b5d1f8",
    "execution_start": 1644175533571,
    "execution_millis": 931,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "aa697592",
    "execution_start": 1644175534510,
    "execution_millis": 1292,
    "cell_id": "ffb56fb6-b3af-4d79-b966-e34e6153de5f",
    "deepnote_cell_type": "code"
   },
   "source": "import argparse, os, logging, random, time\nimport numpy as np\nimport math\nimport time\nimport scipy.sparse\nimport lightgbm as lgb\nimport data_helpers as dh",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e9764fd6",
    "execution_start": 1644177622516,
    "execution_millis": 206,
    "cell_id": "00001-73c9b372-c326-45df-9519-85efa0f62ec7",
    "deepnote_cell_type": "code"
   },
   "source": "import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom sklearn.utils.extmath import softmax\n\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\nfrom torch.optim import Optimizer, AdamW, SGD\n\nimport gc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "58de1d54",
    "execution_start": 1644175537417,
    "execution_millis": 7,
    "deepnote_output_heights": [
     21
    ],
    "cell_id": "00002-6c28609d-59b7-434c-8362-465b50670832",
    "deepnote_cell_type": "code"
   },
   "source": "torch.__version__",
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 5,
     "data": {
      "text/plain": "'1.10.0+cu102'"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "576d90f9",
    "execution_start": 1644175537431,
    "execution_millis": 131013,
    "deepnote_output_heights": [
     21
    ],
    "cell_id": "00003-275def0f-1d1c-4e0b-9796-d8110e943ce5",
    "deepnote_cell_type": "code"
   },
   "source": "torchvision.__version__",
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 6,
     "data": {
      "text/plain": "'0.11.1+cu102'"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "280b5616",
    "execution_start": 1644175537443,
    "execution_millis": 131016,
    "cell_id": "00004-e7c45317-1bb3-47fe-8084-894a6646ef34",
    "deepnote_cell_type": "code"
   },
   "source": "import pdb\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif torch.cuda.is_available():\n    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n    type_prefix = torch.cuda\nelse:\n    type_prefix = torch",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "47cd33c8",
    "execution_start": 1644175537485,
    "execution_millis": 131029,
    "cell_id": "00006-21e4b05e-e064-4754-b322-fac65f70403e",
    "is_code_hidden": true,
    "deepnote_cell_type": "code"
   },
   "source": "def one_hot(y, numslot, mask=None):\n    y_tensor = y.type(type_prefix.LongTensor).reshape(-1, 1)\n    y_one_hot = torch.zeros(y_tensor.size()[0], numslot, device=device, dtype=torch.float32, requires_grad=False).scatter_(1, y_tensor, 1)\n    if mask is not None:\n        y_one_hot = y_one_hot * mask\n    y_one_hot = y_one_hot.reshape(y.shape[0], -1)\n    return y_one_hot",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "91eaa51",
    "execution_start": 1644177414926,
    "execution_millis": 11,
    "cell_id": "00007-208efb08-a5c6-4d84-a616-a8a4d2572c2c",
    "is_code_hidden": false,
    "deepnote_cell_type": "code"
   },
   "source": "class BatchDense(nn.Module):\n    def __init__(self, batch, in_features, out_features, bias_init=None):\n        super(BatchDense, self).__init__()\n        self.batch = batch\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.Tensor(batch, in_features, out_features))\n        self.bias = Parameter(torch.Tensor(batch, 1, out_features))\n        self.reset_parameters(bias_init)\n    def reset_parameters(self, bias_init=None):\n        stdv = math.sqrt(6.0 /(self.in_features + self.out_features))\n        self.weight.data.uniform_(-stdv, stdv)\n        if bias_init is not None:\n            # pdb.set_trace()\n            self.bias.data = torch.from_numpy(np.array(bias_init))\n            \n        else:\n            self.bias.data.fill_(0)\n    def forward(self, x):\n        size = x.size()\n        # Todo: avoid the swap axis\n        x = x.view(x.size(0), self.batch, -1)\n        out = x.transpose(0, 1).contiguous()\n        out = torch.baddbmm(self.bias, out, self.weight)\n        out = out.transpose(0, 1).contiguous()\n        out = out.view(x.size(0), -1)\n        return out",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "18e69f81",
    "execution_start": 1644178739977,
    "execution_millis": 1,
    "cell_id": "00008-63f6b7e0-61f3-415d-8c15-ac6c542c0757",
    "is_code_hidden": false,
    "deepnote_cell_type": "code"
   },
   "source": "class EmbeddingModel(nn.Module):\n    def __init__(self, n_models, max_ntree_per_split, embsize, maxleaf, n_output, out_bias=None, task='regression'):\n        super(EmbeddingModel, self).__init__()\n        self.task = task\n        self.n_models = n_models\n        self.maxleaf = maxleaf\n        self.fcs = nn.ModuleList()\n        self.max_ntree_per_split = max_ntree_per_split\n\n        self.embed_w = Parameter(torch.Tensor(n_models, max_ntree_per_split*maxleaf, embsize))\n        # torch.nn.init.xavier_normal(self.embed_w)\n        stdv = math.sqrt(1.0 /(max_ntree_per_split))\n        self.embed_w.data.normal_(0,stdv) # .uniform_(-stdv, stdv)\n        \n        self.bout = BatchDense(n_models, embsize, 1, out_bias)\n        self.bn = nn.BatchNorm1d(embsize * n_models)\n        self.tanh = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n        # self.output_fc = Dense(n_models * embsize, n_output)\n        self.dropout = torch.nn.Dropout()\n        if task == 'regression':\n            self.criterion = nn.MSELoss()\n        else:\n            self.criterion = nn.BCELoss()\n\n    def batchmul(self, x, models, embed_w, length):\n        out = one_hot(x, length)\n        out = out.view(x.size(0), models, -1)\n        out = out.transpose(0, 1).contiguous()\n        out = torch.bmm(out, embed_w)\n        out = out.transpose(0, 1).contiguous()\n        out = out.view(x.size(0), -1)\n        return out\n        \n    def lastlayer(self, x):\n        out = self.batchmul(x, self.n_models, self.embed_w, self.maxleaf)\n        out = self.bn(out)\n        # out = self.tanh(out)\n        # out = out.view(x.size(0), self.n_models, -1)\n        return out\n    def forward(self, x):\n        out = self.lastlayer(x)\n        out = self.dropout(out)\n        out = out.view(x.size(0), self.n_models, -1)\n        out = self.bout(out)\n        # out = self.output_fc(out)\n        sum_out = torch.sum(out,-1,True)\n        if self.task != 'regression':\n            return self.sigmoid(sum_out), out\n        return sum_out, out\n    \n    def joint_loss(self, out, target, out_inner, target_inner, *args):\n        return nn.MSELoss()(out_inner, target_inner)\n\n    def true_loss(self, out, target):\n        return self.criterion(out, target)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "575bb924",
    "execution_start": 1644180106877,
    "execution_millis": 44,
    "cell_id": "00009-608161e1-eeed-46b5-b7e6-132fe8c9c716",
    "is_code_hidden": false,
    "deepnote_cell_type": "code"
   },
   "source": "def eval_metrics(task, true, pred):\n    if task == 'binary':\n        logloss = sklearn.metrics.log_loss(true.astype(np.float64), pred.astype(np.float64))\n        auc = sklearn.metrics.roc_auc_score(true, pred)\n        # error = 1-sklearn.metrics.accuracy_score(true,(pred+0.5).astype(np.int32))\n        return (logloss, auc)#, error)\n    else:\n        mseloss = sklearn.metrics.mean_squared_error(true, pred)\n        return mseloss\n\ndef EvalTestset(test_x, test_y, model, test_batch_size, test_x_opt=None):\n    test_len = test_x.shape[0]\n    test_num_batch = math.ceil(test_len / test_batch_size)\n    sum_loss = 0.0\n    y_preds = []\n    model.eval()\n    with torch.no_grad():\n        for jdx in range(test_num_batch):\n            tst_st = jdx * test_batch_size\n            tst_ed = min(test_len, tst_st + test_batch_size)\n            inputs = torch.from_numpy(test_x[tst_st:tst_ed].astype(np.float32)).to(device)\n            if test_x_opt is not None:\n                inputs_opt = torch.from_numpy(test_x_opt[tst_st:tst_ed].astype(np.float32)).to(device)\n                outputs = model(inputs, inputs_opt)\n            else:\n                outputs = model(inputs)\n            targets = torch.from_numpy(test_y[tst_st:tst_ed]).to(device)\n            if isinstance(outputs, tuple):\n                outputs = outputs[0]\n            y_preds.append(outputs)\n            loss_tst = model.true_loss(outputs, targets).item()            \n            sum_loss += (tst_ed - tst_st) * loss_tst\n    return sum_loss / test_len, np.concatenate(y_preds, 0)\n\ndef TrainWithLog(loss_dr, loss_init, loss_de, log_freq, test_freq, task, test_batch_size,                \n                train_x, train_y, \n                 train_y_opt, test_x, test_y, model, opt,\n                 epoch, batch_size, n_output, key=\"\",\n                 train_x_opt=None, test_x_opt=None):\n    # trn_writer = tf.summary.FileWriter(summaryPath+plot_title+key+\"_output/train\")\n    # tst_writer = tf.summary.FileWriter(summaryPath+plot_title+key+\"_output/test\")\n    if isinstance(test_x, scipy.sparse.csr_matrix):\n        test_x = test_x.todense()\n    train_len = train_x.shape[0]\n    global_iter = 0\n    trn_batch_size = batch_size\n    train_num_batch = math.ceil(train_len / trn_batch_size)\n    total_iterations = epoch * train_num_batch\n    start_time = time.time()\n    total_time = 0.0\n    min_loss = float(\"Inf\")\n    # min_error = float(\"Inf\")\n    max_auc = 0.0\n    for epoch in range(epoch):\n        shuffled_indices = np.random.permutation(np.arange(train_x.shape[0]))\n        Loss_trn_epoch = 0.0\n        Loss_trn_log = 0.0\n        log_st = 0\n        for local_iter in range(train_num_batch):\n            trn_st = local_iter * trn_batch_size\n            trn_ed = min(train_len, trn_st + trn_batch_size)\n            batch_trn_x = train_x[shuffled_indices[trn_st:trn_ed]]\n            if isinstance(batch_trn_x, scipy.sparse.csr_matrix):\n                batch_trn_x = batch_trn_x.todense()\n            inputs = torch.from_numpy(batch_trn_x.astype(np.float32)).to(device)\n            targets = torch.from_numpy(train_y[shuffled_indices[trn_st:trn_ed],:]).to(device)\n            model.train()\n            if train_x_opt is not None:\n                inputs_opt = torch.from_numpy(train_x_opt[shuffled_indices[trn_st:trn_ed]].astype(np.float32)).to(device)\n                outputs = model(inputs, inputs_opt)\n            else:\n                outputs = model(inputs)\n            opt.zero_grad()\n            if isinstance(outputs, tuple) and train_y_opt is not None:\n                # targets_inner = torch.from_numpy(s_train_y_opt[trn_st:trn_ed,:]).to(device)\n                targets_inner = torch.from_numpy(train_y_opt[shuffled_indices[trn_st:trn_ed],:]).to(device)\n                loss_ratio = loss_init * max(0.3,loss_dr ** (epoch // loss_de))#max(0.5, args.loss_dr ** (epoch // args.loss_de))\n                if len(outputs) == 3:\n                    loss_val = model.joint_loss(outputs[0], targets, outputs[1], targets_inner, loss_ratio, outputs[2])\n                else:\n                    loss_val = model.joint_loss(outputs[0], targets, outputs[1], targets_inner, loss_ratio)\n                loss_val.backward()\n                loss_val = model.true_loss(outputs[0], targets)\n            elif isinstance(outputs, tuple):\n                loss_val = model.true_loss(outputs[0], targets)\n                loss_val.backward()\n            else:\n                loss_val = model.true_loss(outputs, targets)\n                loss_val.backward()\n            opt.step()\n            loss_val = loss_val.item()\n            wandb.log({\"batch loss\":loss_val})\n            global_iter += 1\n            Loss_trn_epoch += (trn_ed - trn_st) * loss_val\n            Loss_trn_log += (trn_ed - trn_st) * loss_val\n            if global_iter % log_freq == 0:\n                print(key+\"Epoch-{:0>3d} {:>5d} Batches, Step {:>6d}, Training Loss: {:>9.6f} (AllAvg {:>9.6f})\"\n                            .format(epoch, local_iter + 1, global_iter, Loss_trn_log/(trn_ed-log_st), Loss_trn_epoch/trn_ed))\n                \n                # trn_summ = tf.Summary()\n                # trn_summ.value.add(tag=args.data+ \"/Train/Loss\", simple_value = Loss_trn_log/(trn_ed-log_st))\n                # trn_writer.add_summary(trn_summ, global_iter)\n                log_st = trn_ed\n                Loss_trn_log = 0.0\n            if global_iter % test_freq == 0 or local_iter == train_num_batch - 1:\n                if model == 'deepgbm' or model == 'd1':\n                    try:\n                        print('Alpha: '+str(model.alpha))\n                        print('Beta: '+str(model.beta))\n                    except:\n                        pass\n                # tst_summ = tf.Summary()\n                torch.cuda.empty_cache()\n                test_loss, pred_y = EvalTestset(test_x, test_y, model, test_batch_size, test_x_opt)\n                wandb.log({\"loss\":test_loss})\n                current_used_time = time.time() - start_time\n                start_time = time.time()\n                wandb.log({\"createdAt\":start_time})\n                total_time += current_used_time\n                remaining_time = (total_iterations - (global_iter) ) * (total_time / (global_iter))\n                if task == 'binary':\n                    metrics = eval_metrics(task, test_y, pred_y)\n                    _, test_auc = metrics\n                    wandb.log({\"test batch auc\":test_auc})\n                    # min_error = min(min_error, test_error)\n                    max_auc = max(max_auc, test_auc)\n                    wandb.log({\"test max auc\":max_auc})\n                    # tst_summ.value.add(tag=args.data+\"/Test/Eval/Error\", simple_value = test_error)\n                    # tst_summ.value.add(tag=args.data+\"/Test/Eval/AUC\", simple_value = test_auc)\n                    # tst_summ.value.add(tag=args.data+\"/Test/Eval/Min_Error\", simple_value = min_error)\n                    # tst_summ.value.add(tag=args.data+\"/Test/Eval/Max_AUC\", simple_value = max_auc)\n                    print(key+\"Evaluate Result:\\nEpoch-{:0>3d} {:>5d} Batches, Step {:>6d}, Testing Loss: {:>9.6f}, Testing AUC: {:8.6f}, Used Time: {:>5.1f}m, Remaining Time: {:5.1f}m\"\n                            .format(epoch, local_iter + 1, global_iter, test_loss, test_auc, total_time/60.0, remaining_time/60.0))\n                else:\n                    print(key+\"Evaluate Result:\\nEpoch-{:0>3d} {:>5d} Batches, Step {:>6d}, Testing Loss: {:>9.6f}, Used Time: {:>5.1f}m, Remaining Time: {:5.1f}m\"\n                            .format(epoch, local_iter + 1, global_iter, test_loss, total_time/60.0, remaining_time/60.0))\n                min_loss = min(min_loss, test_loss)\n                wandb.log({\"test min loss\": min_loss})\n                # tst_summ.value.add(tag=args.data+\"/Test/Loss\", simple_value = test_loss)\n                # tst_summ.value.add(tag=args.data+\"/Test/Min_Loss\", simple_value = min_loss)\n                print(\"-------------------------------------------------------------------------------\")\n                # tst_writer.add_summary(tst_summ, global_iter)\n                # tst_writer.flush()\n        print(\"Best Metric: %s\"%(str(max_auc) if task=='binary' else str(min_loss)))\n        print(\"####################################################################################\")\n    print(\"Final Best Metric: %s\"%(str(max_auc) if task=='binary' else str(min_loss)))\n    return min_loss        \n\ndef GetEmbPred(model, fun, X, test_batch_size):\n    model.eval()\n    tst_len = X.shape[0]\n    test_num_batch = math.ceil(tst_len / test_batch_size)\n    y_preds = []\n    with torch.no_grad():\n        for jdx in range(test_num_batch):\n            tst_st = jdx * test_batch_size\n            tst_ed = min(tst_len, tst_st + test_batch_size)\n            inputs = torch.from_numpy(X[tst_st:tst_ed]).to(device)\n            t_preds = fun(inputs).data.cpu().numpy()\n            y_preds.append(t_preds)\n        y_preds = np.concatenate(y_preds, 0)\n    return y_preds\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7ddb1838",
    "execution_start": 1644175537583,
    "execution_millis": 0,
    "cell_id": "00010-4b9b9799-83d7-4c12-808a-5d7c37215426",
    "deepnote_cell_type": "code"
   },
   "source": "HOME_DIR = os.getcwd()\nDATA_DIR = os.path.join(HOME_DIR, 'data')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "15d6e06e",
    "execution_start": 1644175537584,
    "execution_millis": 3,
    "cell_id": "00011-446232b1-47c8-40ef-9fd4-7e7e7beb29f8",
    "deepnote_cell_type": "code"
   },
   "source": "num_data = dh.load_data('/work/neurotrees/articles code reproduction/DeepGBM/data/data_offline_num')",
   "outputs": [
    {
     "name": "stderr",
     "text": "2022-02-06 19:25:37,559 [INFO] data loaded.\n train_x shape: (3918, 12). train_y shape: (3918, 1).\n test_x shape: (980, 12). test_y shape: (980, 1).\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "14a37530",
    "execution_start": 1644175537585,
    "execution_millis": 0,
    "cell_id": "00012-6bd23b4b-2133-428c-8e1a-684711af74e6",
    "deepnote_cell_type": "code"
   },
   "source": "train_x, train_y, test_x, test_y = num_data",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8e97f491-c718-44a5-9593-4e9d8fa32752",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "22f94691",
    "execution_start": 1644175537586,
    "execution_millis": 130998,
    "deepnote_cell_type": "code"
   },
   "source": "PATH_TO_PICKLE = '/work/neurotrees/experiments/DeepGBM-decomposition/wine-dataset'",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d83cfbaa",
    "execution_start": 1644175537592,
    "execution_millis": 11,
    "cell_id": "00013-1ce90f72-5996-4962-8a80-febf2c3684cb",
    "deepnote_cell_type": "code"
   },
   "source": "import pickle\n\n    ",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "sweep_config = {\n    'method': 'random', #grid, random\n    'metric': {\n      'name': 'loss',\n      'goal': 'minimize'   \n    },\n    'parameters': {\n        'emb_epoch': {\n            'values': [2, 5, 10]\n        },\n        'batch_size': {\n            'values': [256, 128, 64, 32]\n        },\n        \n        'emb_lr': {\n            'values': [1e-2, 1e-3, 1e-4, 3e-4, 3e-5, 1e-5]\n        },\n        \n        'optimizer': {\n            'values': ['adamW', 'sgd']\n        },\n    }\n}",
   "metadata": {
    "cell_id": "4420d812-8e12-409e-8e8c-a1b5b0325cab",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f2cbfc7d",
    "execution_start": 1644176933763,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "sweep_id = wandb.sweep(sweep_config, project=\"deepgbm-wandb\")",
   "metadata": {
    "cell_id": "ac38de1f-eb3f-4a99-950f-038a131376c9",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ec1dc211",
    "execution_start": 1644175964489,
    "execution_millis": 631,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Create sweep with ID: 3qvzlypv\nSweep URL: https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "efeec928",
    "execution_start": 1644178900761,
    "execution_millis": 20,
    "deepnote_output_heights": [
     611
    ],
    "cell_id": "00014-985ed709-dc65-4fd1-8b97-eb8436f2ed84",
    "deepnote_cell_type": "code"
   },
   "source": "# embsize = 20\n# maxleaf = 64\n# task = \"regression\"\n# l2_reg = 1e-6\n# emb_lr = 1e-3\n# emb_epoch = 2\n# batch_size = 512\n# test_batch_size = 100 \n# loss_init = 1.0\n# loss_dr = 0.7\n# loss_de = 2\n# log_freq = 500\n# test_freq = 300\n# key = \"\"\n\n# n_output = train_y.shape[1]\ndef train():\n   # Default values for hyper-parameters we're going to sweep over\n   with open(os.path.join(PATH_TO_PICKLE,'n_models_wine_100.pickle'), 'rb') as f:\n    # Pickle using the highest protocol available.\n    n_models = pickle.load(f)\n    \n   with open(os.path.join(PATH_TO_PICKLE,'max_ntree_per_split_wine_100.pickle'), 'rb') as f:\n      # Pickle using the highest protocol available.\n      max_ntree_per_split = pickle.load(f)\n      \n   with open(os.path.join(PATH_TO_PICKLE,'group_average_wine_100.pickle'), 'rb') as f:\n      # Pickle using the highest protocol available.\n      group_average = pickle.load(f)\n\n   with open(os.path.join(PATH_TO_PICKLE,'leaf_preds_wine_100.pickle'), 'rb') as f:\n      # Pickle using the highest protocol available.\n      leaf_preds = pickle.load(f)\n      \n   with open(os.path.join(PATH_TO_PICKLE,'test_leaf_preds_wine_100.pickle'), 'rb') as f:\n      # Pickle using the highest protocol available.\n      test_leaf_preds = pickle.load(f)\n      \n   with open(os.path.join(PATH_TO_PICKLE,'tree_outputs_wine_100.pickle'), 'rb') as f:\n      # Pickle using the highest protocol available.\n      tree_outputs = pickle.load(f) \n\n   config_defaults = dict(\n      \n      n_models = n_models,\n      max_ntree_per_split = max_ntree_per_split,\n      group_average = group_average,    \n      embsize = 20,\n      maxleaf = 64,\n      task = \"regression\",\n      l2_reg = 1e-6,\n      emb_lr = 1e-3,\n      emb_epoch = 2,\n      batch_size = 512,\n      test_batch_size = 100,\n      loss_init = 1.0,\n      loss_dr = 0.7,\n      loss_de = 2,\n      log_freq = 500,\n      test_freq = 300,\n      key = \"\",\n      n_output = train_y.shape[1]\n      )\n\n\n   # Initialize a new wandb run\n   wandb.init(config=config_defaults)\n    \n   # Config is a variable that holds and saves hyperparameters and inputs\n   config = wandb.config\n    \n    \n\n   #     wandb.log({\"batch loss\":loss.item()})\n   # wandb.log({\"loss\":closs/config.batch_size})\n\n\n   emb_model = EmbeddingModel(config.n_models, config.max_ntree_per_split, \n                              config.embsize,\n                              config.maxleaf+1, config.n_output,\n                              config.group_average, task=config.task).float().to(device)\n   if config.optimizer=='sgd':\n         opt = SGD(emb_model.parameters(),lr=config.emb_lr, momentum=0.9)\n   elif config.optimizer=='adamW':\n         opt = AdamW(emb_model.parameters(),lr=config.emb_lr, weight_decay=config.l2_reg)\n\n   tree_outputs = np.asarray(tree_outputs).reshape((config.n_models, \n                  leaf_preds.shape[0])).transpose((1,0))\n\n   TrainWithLog(config.loss_dr, config.loss_init, config.loss_de, config.log_freq, \n               config.test_freq, \n               config.task, config.test_batch_size,\n               leaf_preds, train_y, tree_outputs,\n               test_leaf_preds, test_y, emb_model, opt,\n               config.emb_epoch, config.batch_size, config.n_output, config.key+\"emb-\")\n\n\n   output_w = emb_model.bout.weight.data.cpu().numpy().reshape(config.n_models*config.embsize, config.n_output)\n   output_b = np.array(emb_model.bout.bias.data.cpu().numpy().sum())\n   train_embs = GetEmbPred(emb_model, emb_model.lastlayer, leaf_preds,\n                      config.test_batch_size)\n   del tree_outputs, leaf_preds, test_leaf_preds\n   gc.collect();\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train()",
   "metadata": {
    "cell_id": "2ae137ea-535a-4b16-8963-74cd9afbc98b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "661bb006",
    "execution_start": 1644178906727,
    "execution_millis": 10710,
    "deepnote_output_heights": [
     21,
     40,
     81,
     21,
     40,
     252,
     611
    ],
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:hxua7j08) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1760... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n</div><div class=\"wandb-col\">\n</div></div>\nSynced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">eternal-sweep-6</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/hxua7j08\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/hxua7j08</a><br/>\nFind logs at: <code>./wandb/run-20220206_202039-hxua7j08/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:hxua7j08). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/hxua7j08\" target=\"_blank\">eternal-sweep-6</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 27.826828, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 27.826828470035476\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 17.162858, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 17.162857834173707\n####################################################################################\nFinal Best Metric: 17.162857834173707\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a7c22e3f",
    "execution_start": 1644179102262,
    "execution_millis": 294712,
    "cell_id": "00015-c5092149-13bf-496d-bd78-235ee088dbc5",
    "deepnote_output_heights": [
     null,
     40,
     40,
     40,
     350,
     40,
     40,
     81,
     40,
     350,
     40,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350,
     null,
     40,
     null,
     40,
     350
    ],
    "deepnote_cell_type": "code"
   },
   "source": "wandb.agent(sweep_id, train)",
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fcen0bvl with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/fcen0bvl\" target=\"_blank\">dark-sweep-2</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss:  8.830520, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 8.830519637283015\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss:  4.390103, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 4.390102590833392\n####################################################################################\nFinal Best Metric: 4.390102590833392\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1853... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▆▆▆▆▆▄▄▄▄▃▃▃▃▂▂▂▃▂▂▂▂▂▁▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▂</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>8.74636</td></tr><tr><td>test batch loss</td><td>4.3901</td></tr><tr><td>test min loss</td><td>4.3901</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">dark-sweep-2</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/fcen0bvl\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/fcen0bvl</a><br/>\nFind logs at: <code>./wandb/run-20220206_202502-fcen0bvl/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2yxraxip with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/2yxraxip\" target=\"_blank\">serene-sweep-3</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 17.439319, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 17.43931939650555\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss:  9.659047, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 9.659047496562101\n####################################################################################\nFinal Best Metric: 9.659047496562101\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1891... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇▇▆█▆▅▆▆▅▄▅▅▄▄▄▄▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>17.5583</td></tr><tr><td>test batch loss</td><td>9.65905</td></tr><tr><td>test min loss</td><td>9.65905</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">serene-sweep-3</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/2yxraxip\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/2yxraxip</a><br/>\nFind logs at: <code>./wandb/run-20220206_202518-2yxraxip/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 216xymx7 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/216xymx7\" target=\"_blank\">young-sweep-4</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 20.528013, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 20.52801311259367\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss: 13.369922, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 13.36992174265336\n####################################################################################\nFinal Best Metric: 13.36992174265336\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1929... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆▆▆▆█▄▆▄▅▄▅▆▄▃▃▄▄▄▄▂▂▃▃▃▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁▃</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>25.36608</td></tr><tr><td>test batch loss</td><td>13.36992</td></tr><tr><td>test min loss</td><td>13.36992</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">young-sweep-4</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/216xymx7\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/216xymx7</a><br/>\nFind logs at: <code>./wandb/run-20220206_202528-216xymx7/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vvhjigvj with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/vvhjigvj\" target=\"_blank\">dazzling-sweep-5</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 18.200510, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 18.200510375353755\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss:  9.549342, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 9.549341688350756\n####################################################################################\nFinal Best Metric: 9.549341688350756\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1967... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇█▆█▇▆▇▅▄▅▅▄▄▂▄▃▃▄▄▃▃▃▃▂▂▃▂▁▃▂▂▂▂▂▁▂▁▁▁▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>10.05197</td></tr><tr><td>test batch loss</td><td>9.54934</td></tr><tr><td>test min loss</td><td>9.54934</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">dazzling-sweep-5</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/vvhjigvj\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/vvhjigvj</a><br/>\nFind logs at: <code>./wandb/run-20220206_202548-vvhjigvj/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: smwbysht with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/smwbysht\" target=\"_blank\">icy-sweep-6</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    16 Batches, Step     16, Testing Loss: 34.706379, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 34.70637928709692\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    16 Batches, Step     32, Testing Loss: 27.569772, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 27.569772253231125\n####################################################################################\nFinal Best Metric: 27.569772253231125\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2006... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇▅█▆█▆▇▇▇▆▆▅▆▆▆▆▄▄▄▃▃▄▃▂▁▂▃▄▃▁▂▄</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>40.98534</td></tr><tr><td>test batch loss</td><td>27.56977</td></tr><tr><td>test min loss</td><td>27.56977</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">icy-sweep-6</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/smwbysht\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/smwbysht</a><br/>\nFind logs at: <code>./wandb/run-20220206_202608-smwbysht/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ok1ua4dv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/ok1ua4dv\" target=\"_blank\">helpful-sweep-7</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss:  9.839742, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 9.839742076640226\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss:  4.360511, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 4.360510972081398\n####################################################################################\nFinal Best Metric: 4.360510972081398\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2044... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▅▇█▄▄▃▃▃▃▃▂▃▂▂▂▃▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>4.19167</td></tr><tr><td>test batch loss</td><td>4.36051</td></tr><tr><td>test min loss</td><td>4.36051</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">helpful-sweep-7</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/ok1ua4dv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/ok1ua4dv</a><br/>\nFind logs at: <code>./wandb/run-20220206_202623-ok1ua4dv/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t2xy2poy with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/t2xy2poy\" target=\"_blank\">stellar-sweep-8</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss: 13.715305, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 13.715304744486906\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss:  5.912545, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 5.912545330670415\n####################################################################################\nFinal Best Metric: 5.912545330670415\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2082... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>██▆██▇▅▅▅▅▆▅▄▅▄▄▃▂▃▃▅▃▂▃▃▄▂▃▂▁▂▂▂▃▁▁▂▂▁▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>8.82458</td></tr><tr><td>test batch loss</td><td>5.91255</td></tr><tr><td>test min loss</td><td>5.91255</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">stellar-sweep-8</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/t2xy2poy\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/t2xy2poy</a><br/>\nFind logs at: <code>./wandb/run-20220206_202644-t2xy2poy/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ux56dvfg with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/ux56dvfg\" target=\"_blank\">sweepy-sweep-9</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 27.702553, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 27.70255287326112\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 16.673847, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 16.673847198486328\n####################################################################################\nFinal Best Metric: 16.673847198486328\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2120... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇▇█▆██▆▆▇▇▅▇▆▆▄▆▄▄▅▄▄▃▃▄▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>17.98993</td></tr><tr><td>test batch loss</td><td>16.67385</td></tr><tr><td>test min loss</td><td>16.67385</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">sweepy-sweep-9</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/ux56dvfg\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/ux56dvfg</a><br/>\nFind logs at: <code>./wandb/run-20220206_202704-ux56dvfg/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fhac6xfj with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/fhac6xfj\" target=\"_blank\">eager-sweep-10</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 27.878355, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 27.878354598064814\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 16.856367, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 16.856366644100266\n####################################################################################\nFinal Best Metric: 16.856366644100266\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2158... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇▇▇█▇▇▇▇█▆█▆▆▄▅▆▅▅▅▆▄▃▃▄▃▂▂▃▂▂▂▂▂▁▁▂▁▁▁▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>23.4097</td></tr><tr><td>test batch loss</td><td>16.85637</td></tr><tr><td>test min loss</td><td>16.85637</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">eager-sweep-10</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/fhac6xfj\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/fhac6xfj</a><br/>\nFind logs at: <code>./wandb/run-20220206_202714-fhac6xfj/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1b5f3rz5 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/1b5f3rz5\" target=\"_blank\">breezy-sweep-11</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 23.145026, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 23.145026187507472\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss: 12.960984, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 12.960984405206174\n####################################################################################\nFinal Best Metric: 12.960984405206174\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2196... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆▇█▆▆▅▅▅▅▆▅▅▅▄▄▄▄▃▃▃▂▄▃▂▃▂▂▂▂▃▃▂▂▂▂▂▂▁▂▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>14.89263</td></tr><tr><td>test batch loss</td><td>12.96098</td></tr><tr><td>test min loss</td><td>12.96098</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">breezy-sweep-11</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/1b5f3rz5\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/1b5f3rz5</a><br/>\nFind logs at: <code>./wandb/run-20220206_202730-1b5f3rz5/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z6b1ey2m with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/z6b1ey2m\" target=\"_blank\">kind-sweep-12</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss:  9.009322, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 9.009322458383988\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss:  4.240288, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 4.24028789267248\n####################################################################################\nFinal Best Metric: 4.24028789267248\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2234... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆█▆▆▅▅▅▅▃▃▃▂▃▃▃▂▂▂▂▂▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>5.02417</td></tr><tr><td>test batch loss</td><td>4.24029</td></tr><tr><td>test min loss</td><td>4.24029</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">kind-sweep-12</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/z6b1ey2m\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/z6b1ey2m</a><br/>\nFind logs at: <code>./wandb/run-20220206_202750-z6b1ey2m/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r4wuao77 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/r4wuao77\" target=\"_blank\">comfy-sweep-13</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000   123 Batches, Step    123, Testing Loss:  8.607315, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 8.607315267835345\n####################################################################################\nemb-Evaluate Result:\nEpoch-001   123 Batches, Step    246, Testing Loss:  4.402598, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 4.402598055041566\n####################################################################################\nFinal Best Metric: 4.402598055041566\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2272... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▇▅▆▅▄▄▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>4.66091</td></tr><tr><td>test batch loss</td><td>4.4026</td></tr><tr><td>test min loss</td><td>4.4026</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">comfy-sweep-13</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/r4wuao77\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/r4wuao77</a><br/>\nFind logs at: <code>./wandb/run-20220206_202805-r4wuao77/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hpdu8e0l with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/hpdu8e0l\" target=\"_blank\">lively-sweep-14</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 26.685686, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 26.685685761120855\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 16.787947, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 16.78794712923011\n####################################################################################\nFinal Best Metric: 16.78794712923011\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2310... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>██▆█▇▅▆▅▆▅▆▅▅▅▄▄▄▄▃▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▃▁▂▁▁▂</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>21.61503</td></tr><tr><td>test batch loss</td><td>16.78795</td></tr><tr><td>test min loss</td><td>16.78795</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">lively-sweep-14</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/hpdu8e0l\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/hpdu8e0l</a><br/>\nFind logs at: <code>./wandb/run-20220206_202816-hpdu8e0l/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f55uz4xh with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/f55uz4xh\" target=\"_blank\">polished-sweep-15</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 19.157319, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 19.15731869911661\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss: 10.589746, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 10.58974569671008\n####################################################################################\nFinal Best Metric: 10.58974569671008\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2348... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇▅█▇▇█▆▅▆▅▅▅▄▄▃▄▃▃▃▃▃▄▂▃▂▃▃▂▂▂▃▂▂▂▂▂▂▂▂▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>7.71197</td></tr><tr><td>test batch loss</td><td>10.58975</td></tr><tr><td>test min loss</td><td>10.58975</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">polished-sweep-15</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/f55uz4xh\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/f55uz4xh</a><br/>\nFind logs at: <code>./wandb/run-20220206_202831-f55uz4xh/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f7dumqd8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/f7dumqd8\" target=\"_blank\">young-sweep-16</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 22.281786, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 22.281786393146124\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss: 13.911987, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 13.911987168448311\n####################################################################################\nFinal Best Metric: 13.911987168448311\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2386... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆▇█▇▆▆▆▆▅▃▃▃▄▄▃▃▄▃▃▄▃▃▄▂▂▂▁▂▂▂▂▂▁▁▂▂▂▁▁▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>18.96733</td></tr><tr><td>test batch loss</td><td>13.91199</td></tr><tr><td>test min loss</td><td>13.91199</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">young-sweep-16</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/f7dumqd8\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/f7dumqd8</a><br/>\nFind logs at: <code>./wandb/run-20220206_202841-f7dumqd8/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sihgo8h8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/sihgo8h8\" target=\"_blank\">smart-sweep-17</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 23.503257, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 23.5032569729552\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss: 14.909015, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 14.909014974321638\n####################################################################################\nFinal Best Metric: 14.909014974321638\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2424... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▇▄▅▅▅▄▄▅▅▄▃▄▃▃▂▃▃▃▂▂▃▃▃▂▂▂▂▂▁▂▁▂▁▁▂▂▂▁▂</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>23.18174</td></tr><tr><td>test batch loss</td><td>14.90901</td></tr><tr><td>test min loss</td><td>14.90901</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">smart-sweep-17</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/sihgo8h8\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/sihgo8h8</a><br/>\nFind logs at: <code>./wandb/run-20220206_202852-sihgo8h8/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: grdobgfi with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/grdobgfi\" target=\"_blank\">light-sweep-18</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    16 Batches, Step     16, Testing Loss: 34.677828, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 34.677827562604634\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    16 Batches, Step     32, Testing Loss: 28.301034, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 28.301033759603694\n####################################################################################\nFinal Best Metric: 28.301033759603694\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2462... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▇▇▇▇█▇█▇▆▆▅▄▆▅▆▇▅▅▅▄▄▅▃▄▃▄▄▃▂▂▂▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>31.2646</td></tr><tr><td>test batch loss</td><td>28.30103</td></tr><tr><td>test min loss</td><td>28.30103</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">light-sweep-18</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/grdobgfi\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/grdobgfi</a><br/>\nFind logs at: <code>./wandb/run-20220206_202902-grdobgfi/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w07k8zhz with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/w07k8zhz\" target=\"_blank\">dandy-sweep-19</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    31 Batches, Step     31, Testing Loss: 29.640209, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 29.64020923692353\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    31 Batches, Step     62, Testing Loss: 21.519539, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 21.519538957245494\n####################################################################################\nFinal Best Metric: 21.519538957245494\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2500... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▆█▆▆▅▅▅▄▅▄▄▄▄▃▄▄▃▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▁▁▂▂▁▂▂▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>26.86617</td></tr><tr><td>test batch loss</td><td>21.51954</td></tr><tr><td>test min loss</td><td>21.51954</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">dandy-sweep-19</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/w07k8zhz\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/w07k8zhz</a><br/>\nFind logs at: <code>./wandb/run-20220206_202931-w07k8zhz/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hfs8fgi9 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/hfs8fgi9\" target=\"_blank\">floral-sweep-20</a></strong> to <a href=\"https://wandb.ai/iloncka/deepgbm-wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\nSweep page: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/sweeps/3qvzlypv</a><br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "emb-Evaluate Result:\nEpoch-000    62 Batches, Step     62, Testing Loss: 20.559949, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 20.559948823889908\n####################################################################################\nemb-Evaluate Result:\nEpoch-001    62 Batches, Step    124, Testing Loss: 11.872314, Used Time:   0.0m, Remaining Time:   0.0m\n-------------------------------------------------------------------------------\nBest Metric: 11.872313908168248\n####################################################################################\nFinal Best Metric: 11.872313908168248\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2538... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▇▇▆▇▆▆▅▅▆▅▆▆▅▆▄▅▅▄▄▅▃▃▃▄▃▃▃▃▂▂▃▂▃▂▂▃▂▁▁</td></tr><tr><td>test batch loss</td><td>█▁</td></tr><tr><td>test min loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>8.61398</td></tr><tr><td>test batch loss</td><td>11.87231</td></tr><tr><td>test min loss</td><td>11.87231</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">floral-sweep-20</strong>: <a href=\"https://wandb.ai/iloncka/deepgbm-wandb/runs/hfs8fgi9\" target=\"_blank\">https://wandb.ai/iloncka/deepgbm-wandb/runs/hfs8fgi9</a><br/>\nFind logs at: <code>./wandb/run-20220206_202943-hfs8fgi9/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=de072003-a9db-4342-8067-19a4b45feff1' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "a57545f1-6b2d-4e28-a5cd-a8ee694d6617",
  "deepnote_execution_queue": []
 }
}